{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MDP:\n",
    "  \"\"\"A Markov Decision Process.\"\"\"\n",
    "\n",
    "  @property\n",
    "  @abc.abstractmethod\n",
    "  def state_space(self):\n",
    "    \"\"\"Representation of the MDP state set.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Override me\")\n",
    "\n",
    "  @property\n",
    "  @abc.abstractmethod\n",
    "  def action_space(self):\n",
    "    \"\"\"Representation of the MDP action set.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Override me\")\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def get_transition_distribution(self, state, action):\n",
    "    \"\"\"Return a distribution over next states.\n",
    "\n",
    "    The form of this distribution will vary, e.g., depending\n",
    "    on whether the MDP has discrete or continuous states.\n",
    "\n",
    "    Args:\n",
    "      state: A current state.\n",
    "      action: An action.\n",
    "\n",
    "    Returns:\n",
    "      next_state_distribution: Distribution over next states.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Override me\")\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def get_reward(self, state, action, next_state):\n",
    "    \"\"\"Return (deterministic) reward for executing action\n",
    "    in state.\n",
    "\n",
    "    Args:\n",
    "      state: A current state.\n",
    "      action: An action.\n",
    "      next_state: A next state.\n",
    "\n",
    "    Returns:\n",
    "      reward: Single time step reward.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Override me\")\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def state_is_terminal(self, state):\n",
    "    \"\"\"Designate certain states as terminal (done) states.\n",
    "\n",
    "    Args:\n",
    "      state: A state.\n",
    "\n",
    "    Returns:\n",
    "      is_terminal : A bool.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Override me\")\n",
    "\n",
    "\n",
    "# Example MDPs for tests\n",
    "class Debug1DGridMDP(MDP):\n",
    "  \"\"\"A 1D grid MDP for debugging. The grid is 1x5\n",
    "  and the agent is meant to start off in the middle.\n",
    "  There is +10 reward on the rightmost square, -10 on\n",
    "  the left. Actions are left and right. An action effect\n",
    "  is reversed with 10% probability.\n",
    "  \"\"\"\n",
    "  @property\n",
    "  def state_space(self):\n",
    "    return [0, 1, 2, 3, 4]  # position in grid\n",
    "\n",
    "  @property\n",
    "  def action_space(self):\n",
    "    return [0, 1]  # left, right\n",
    "\n",
    "  def get_transition_distribution(self, state, action):\n",
    "    # Discrete distributions, represented with a dict\n",
    "    # mapping next states to probs.\n",
    "    delta = 1 if action == 1 else -1\n",
    "    intended_effect = min(max(state + delta, 0), 4)\n",
    "    opposite_effect = min(max(state - delta, 0), 4)\n",
    "    assert (intended_effect != opposite_effect)\n",
    "    return {intended_effect: 0.9, opposite_effect: 0.1}\n",
    "\n",
    "  def get_reward(self, state, action, next_state):\n",
    "    if next_state == 0:\n",
    "      return -10\n",
    "    if next_state == 4:\n",
    "      return 10\n",
    "    return -1  # living penalty\n",
    "\n",
    "  def state_is_terminal(self, state):\n",
    "    return state in [0, 4]\n",
    "\n",
    "\n",
    "class GridMDP(MDP):\n",
    "  \"\"\"A 2D grid MDP.\n",
    "\n",
    "  Action effects are stochastic: with 0.75 probability, the action has\n",
    "  the intended effect, otherwise a random local move is taken.\n",
    "\n",
    "  The grid is determined by a 2D array of obstacles.\n",
    "\n",
    "  Rewards are \n",
    "   - +1 for reaching the goal\n",
    "   - -1 for reaching a trap\n",
    "   - -1e-3 for each step\n",
    "  \"\"\"\n",
    "\n",
    "  @property\n",
    "  def obstacles(self):\n",
    "    return np.array([\n",
    "      [0, 0, 1, 1, 0],\n",
    "      [0, 0, 1, 0, 0],\n",
    "      [1, 0, 0, 0, 1],\n",
    "      [1, 1, 0, 0, 1],\n",
    "      [0, 0, 0, 0, 0],\n",
    "    ])\n",
    "\n",
    "  @property\n",
    "  def goal(self):\n",
    "    return (4, 4)\n",
    "\n",
    "  @property\n",
    "  def traps(self):\n",
    "    return [(0, 1), (4, 0)]\n",
    "\n",
    "  @property\n",
    "  def goal_reward(self):\n",
    "    return 1.\n",
    "\n",
    "  @property\n",
    "  def trap_reward(self):\n",
    "    return -1.\n",
    "\n",
    "  @property\n",
    "  def living_reward(self):\n",
    "    return -1e-3\n",
    "\n",
    "  @property\n",
    "  def height(self):\n",
    "    return self.obstacles.shape[0]\n",
    "\n",
    "  @property\n",
    "  def width(self):\n",
    "    return self.obstacles.shape[1]\n",
    "\n",
    "  @property\n",
    "  def state_space(self):\n",
    "    return [(r, c) for r in range(self.height) for c in range(self.width)]\n",
    "\n",
    "  @property\n",
    "  def action_space(self):\n",
    "    return ['up', 'down', 'left', 'right']\n",
    "\n",
    "  def action_to_delta(self, action):\n",
    "    return {\n",
    "      'up': (-1, 0),  # up,\n",
    "      'down': (1, 0),  # down,\n",
    "      'left': (0, -1),  # left,\n",
    "      'right': (0, 1),  # right,\n",
    "    }[action]\n",
    "\n",
    "  def get_transition_distribution(self, state, action):\n",
    "    # Discrete distributions, represented with a dict\n",
    "    # mapping next states to probs.\n",
    "    row, col = state\n",
    "\n",
    "    next_state_distribution = defaultdict(float)\n",
    "\n",
    "    for a in self.action_space:\n",
    "      dr, dc = self.action_to_delta(a)\n",
    "      r, c = row + dr, col + dc\n",
    "      # Stay in place if out of bounds or obstacle\n",
    "      if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "        r, c = row, col\n",
    "      elif self.obstacles[r, c]:\n",
    "        r, c = row, col\n",
    "      if a == action:\n",
    "        p = 0.75\n",
    "      else:\n",
    "        p = 0.25 / (len(self.action_space) - 1)\n",
    "      next_state_distribution[(r, c)] += p\n",
    "\n",
    "    return next_state_distribution\n",
    "\n",
    "  def get_reward(self, state, action, next_state):\n",
    "    if next_state == self.goal:\n",
    "      return self.goal_reward\n",
    "    elif next_state in self.traps:\n",
    "      return self.trap_reward\n",
    "    return self.living_reward\n",
    "\n",
    "  def state_is_terminal(self, state):\n",
    "    return state in [self.goal] + self.traps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation\n",
    "Complete the following implementation of iterative policy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(pi, mdp, max_num_iterations=1000, change_threshold=0.0001, gamma=1.0):\n",
    "  \"\"\"Computes a value function for a policy in an MDP.\n",
    "\n",
    "  Assumes that mdp has finite state and action spaces.\n",
    "\n",
    "  Args:\n",
    "    pi: A dict that maps a state to an action in the MDP.\n",
    "    mdp : An MDP.\n",
    "    max_num_iterations: An int representing the maximum number of\n",
    "      iterations to run iteration before giving up.\n",
    "    change_threshold: A float used to determine when iteration\n",
    "      has converged and it is safe to terminate.\n",
    "    gamma: A float temporal discount factor between 0 and 1.\n",
    "\n",
    "  Returns:\n",
    "    value_function: A dict from states to values.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = Debug1DGridMDP()\n",
    "good_policy = {s: 1 for s in mdp.state_space}\n",
    "expected_V = {0: 0.0, 1: 5.585314516769024, 2: 8.317064005349863, 3: 9.731701612974335, 4: 0.0}\n",
    "V = evaluate_policy(good_policy, mdp)\n",
    "assert all(abs(expected_V[s] - V[s]) < 1e-5 for s in mdp.state_space)\n",
    "\n",
    "mdp = Debug1DGridMDP()\n",
    "bad_policy = {s: 0 for s in mdp.state_space}\n",
    "V = evaluate_policy(bad_policy, mdp)\n",
    "expected_V = {0: 0.0, 1: -10.219505953464065, 2: -11.195109614239257, 3: -9.975553581176575, 4: 0.0}\n",
    "assert all(abs(expected_V[s] - V[s]) < 1e-5 for s in mdp.state_space)\n",
    "\n",
    "mdp = Debug1DGridMDP()\n",
    "mixed_policy = {s: 1 if s >= 2 else 0 for s in mdp.state_space}\n",
    "V = evaluate_policy(mixed_policy, mdp)\n",
    "expected_V = {0: 0.0, 1: -8.422229, 2: 6.7777, 3: 9.577771, 4: 0.0}\n",
    "assert all(abs(expected_V[s] - V[s]) < 1e-5 for s in mdp.state_space)\n",
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "Complete the following implementation of value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, max_num_iterations=1000, change_threshold=0.0001, gamma=0.99, initial_Q=None, terminal_state_reward_fn=None, print_every=None, render_value_functions=False):\n",
    "  \"\"\"Run value iteration for a certain number of iterations or until\n",
    "  the max change between iterations is below a threshold.\n",
    "\n",
    "  MDP must have discrete state and action spaces.\n",
    "\n",
    "  Gamma is the temporal discount factor.\n",
    "\n",
    "  Returns a policy defined over non-terminal states.\n",
    "\n",
    "  Args:\n",
    "    mdp: An MDP.\n",
    "    max_num_iterations: An int representing the maximum number of\n",
    "        iterations to run value iteration before giving up.\n",
    "    change_threshold: A float used to determine when value iteration\n",
    "        has converged and it is safe to terminate.\n",
    "    gamma: A float temporal discount factor between 0 and 1.\n",
    "\n",
    "  Returns: \n",
    "    pi: A dict; pi[state] -> action is the policy.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert value_iteration(Debug1DGridMDP()) == {1: 1, 2: 1, 3: 1}\n",
    "assert value_iteration(GridMDP()) == {(0, 0): \"down\", (0, 2): \"right\", (0, 3): \"down\", (0, 4): \"down\", (1, 0): \"right\", (1, 1): \"down\", (1, 2): \"down\", (1, 3): \"down\", (1, 4): \"left\", (2, 0): \"right\", (2, 1): \"right\", (2, 2): \"down\", (2, 3): \"left\", (2, 4): \"left\", (3, 0): \"right\", (3, 1): \"right\", (3, 2): \"down\", (3, 3): \"down\", (3, 4): \"down\", (4, 1): \"right\", (4, 2): \"right\", (4, 3): \"right\"}\n",
    "print('Tests passed.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
