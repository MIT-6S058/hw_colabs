{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def state_space(self):\n",
    "        \"\"\"Representation of the MDP state set.\n",
    "\n",
    "        Unless otherwise stated, assume this is a set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def action_space(self):\n",
    "        \"\"\"Representation of the MDP action set.\n",
    "\n",
    "        Unless otherwise stated, assume this is a set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        \"\"\"Gamma, defaults to 1.\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        \"\"\"H, defaults to inf.\n",
    "        \"\"\"\n",
    "        return float(\"inf\")\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        \"\"\"Designate certain states as terminal (done) states.\n",
    "\n",
    "        Defaults to False.\n",
    "\n",
    "        Args:\n",
    "            state: A state.\n",
    "\n",
    "        Returns:\n",
    "            is_terminal : A bool.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Return (deterministic) reward for executing action\n",
    "        in state.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "            next_state: A next state.\n",
    "\n",
    "        Returns:\n",
    "            reward : Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        \"\"\"Return a distribution over next states.\n",
    "\n",
    "        Unless otherwise stated, assume that this returns\n",
    "        a dictionary mapping states to probabilities. For\n",
    "        example, if the state space were {0, 1, 2}, then\n",
    "        this function might return {0: 0.3, 1: 0.2, 2: 0.5}.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "\n",
    "        Returns:\n",
    "            next_state_distribution: Distribution over next states.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def sample_next_state(self, state, action, rng=np.random):\n",
    "        \"\"\"Sample a next state from the transition distribution.\n",
    "\n",
    "        This function may be overwritten by subclasses when the explicit\n",
    "        distribution is too large to enumerate.\n",
    "\n",
    "        Args:\n",
    "            state: A state from the state space.\n",
    "            action: An action from the action space.\n",
    "            rng: A random number generator.\n",
    "\n",
    "        Returns:\n",
    "            next_state: A sampled next state from the state space.\n",
    "        \"\"\"\n",
    "        next_state_dist = self.get_transition_distribution(state, action)\n",
    "        next_states, probs = zip(*next_state_dist.items())\n",
    "        next_state_index = rng.choice(len(next_states), p=probs)\n",
    "        next_state = next_states[next_state_index]\n",
    "        return next_state\n",
    "\n",
    "\n",
    "class SingleRowMDP(MDP):\n",
    "    \"\"\"A 1D grid MDP for debugging. The grid is 1x5\n",
    "    and the agent is meant to start off in the middle.\n",
    "    There is +10 reward on the rightmost square, -10 on\n",
    "    the left. Actions are left and right. An action effect\n",
    "    is reversed with 10% probability.\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return {0, 1, 2, 3, 4}  # position in grid\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {0, 1}  # left, right\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Discrete distributions, represented with a dict\n",
    "        # mapping next states to probs.\n",
    "        delta = 1 if action == 1 else -1\n",
    "        intended_effect = min(max(state + delta, 0), 4)\n",
    "        opposite_effect = min(max(state - delta, 0), 4)\n",
    "        assert (intended_effect != opposite_effect)\n",
    "        return {intended_effect: 0.9, opposite_effect: 0.1}\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if next_state == 0:\n",
    "          return -10\n",
    "        if next_state == 4:\n",
    "          return 10\n",
    "        return -1  # living penalty\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        return state in {0, 4}\n",
    "\n",
    "\n",
    "class MarshmallowMDP(MDP):\n",
    "    \"\"\"The Marshmallow MDP described in lecture.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        # (hunger level, marshmallow remains)\n",
    "        return {(h, m) for h in {0, 1, 2} for m in {True, False}}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {\"eat\", \"wait\"}\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        return 4\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        next_hunger_level = next_state[0]\n",
    "        return -(next_hunger_level**2)\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Update marshmallow deterministically\n",
    "        if action == \"eat\":\n",
    "            next_m = False\n",
    "        else:\n",
    "            next_m = state[1]\n",
    "\n",
    "        # Initialize next state distribution dict\n",
    "        # Any state not included assumed to have 0 prob\n",
    "        dist = defaultdict(float)\n",
    "\n",
    "        # Update hunger\n",
    "        if action == \"wait\" or state[1] == False:\n",
    "            # With 0.75 probability, hunger stays the same\n",
    "            dist[(state[0], next_m)] += 0.75\n",
    "            # With 0.25 probability, hunger increases by 1\n",
    "            dist[(min(state[0] + 1, 2), next_m)] += 0.25\n",
    "\n",
    "        else:\n",
    "            assert action == \"eat\" and state[1] == True\n",
    "            # Hunger deterministically set to 1 after eating\n",
    "            dist[(0, next_m)] = 1.0\n",
    "\n",
    "        return dist\n",
    "\n",
    "\n",
    "class ZitsMDP(MDP):\n",
    "    \"\"\"The Zits MDP described in lecture.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return {0, 1, 2, 3, 4}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {\"apply\", \"sleep\"}\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return 0.9\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if action == \"apply\":\n",
    "            return -1 - next_state\n",
    "        assert action == \"sleep\"\n",
    "        return -next_state\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        if action == \"apply\":\n",
    "            return {\n",
    "                0: 0.8,\n",
    "                4: 0.2\n",
    "            }\n",
    "        assert action == \"sleep\"\n",
    "        return {\n",
    "            min(state + 1, 4): 0.4,\n",
    "            max(state - 1, 0): 0.6\n",
    "        }\n",
    "\n",
    "\n",
    "class ChaseMDP(MDP):\n",
    "    \"\"\"A 2D grid bunny chasing MDP.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "        return np.zeros((2, 3))  # by default, 2x3 grid with no obstacles\n",
    "\n",
    "    @property\n",
    "    def goal_reward(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def living_reward(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "        return self.obstacles.shape[0]\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "        return self.obstacles.shape[1]\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        pos = [(r, c) for r in range(self.height) for c in range(self.width)]\n",
    "        return {(p1, p2) for p1 in pos for p2 in pos}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {'up', 'down', 'left', 'right'}\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return 0.9\n",
    "\n",
    "    def action_to_delta(self, action):\n",
    "        return {\n",
    "            'up': (-1, 0),  # up,\n",
    "            'down': (1, 0),  # down,\n",
    "            'left': (0, -1),  # left,\n",
    "            'right': (0, 1),  # right,\n",
    "        }[action]\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Discrete distributions, represented with a dict\n",
    "        # mapping next states to probs.\n",
    "        next_state_dist = defaultdict(float)\n",
    "\n",
    "        agent_pos, goal_pos = state\n",
    "\n",
    "        # Get next agent state\n",
    "        row, col = agent_pos\n",
    "        dr, dc = self.action_to_delta(action)\n",
    "        r, c = row + dr, col + dc\n",
    "        # Stay in place if out of bounds or obstacle\n",
    "        if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "            r, c = row, col\n",
    "        elif self.obstacles[r, c]:\n",
    "            r, c = row, col\n",
    "        next_agent_pos = (r, c)\n",
    "\n",
    "        # Get next bunny state\n",
    "        # Stay in same place with probability 0.5\n",
    "        next_state_dist[(next_agent_pos, goal_pos)] += 0.5\n",
    "        # Otherwise move\n",
    "        row, col = goal_pos\n",
    "        for (dr, dc) in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            r, c = row + dr, col + dc\n",
    "            # Stay in place if out of bounds or obstacle\n",
    "            if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "                r, c = row, col\n",
    "            elif self.obstacles[r, c]:\n",
    "                r, c = row, col\n",
    "            next_goal_pos = (r, c)\n",
    "            next_state_dist[(next_agent_pos, next_goal_pos)] += 0.5*0.25\n",
    "\n",
    "        return next_state_dist\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        agent_pos, goal_pos = next_state\n",
    "        if agent_pos == goal_pos:\n",
    "            return self.goal_reward\n",
    "        return self.living_reward\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        agent_pos, goal_pos = state\n",
    "        return agent_pos == goal_pos\n",
    "\n",
    "\n",
    "class LargeChaseMDP(ChaseMDP):\n",
    "    \"\"\"A larger 2D grid bunny chasing MDP.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "        return np.array([\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "            [0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait, Bellman, Backup!\n",
    "Complete the implementation of the bellman backup for an infinite or indefinite horizon MDP.\n",
    "\n",
    "For reference, our solution is **12** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_backup(s, V, mdp):\n",
    "  \"\"\"Look ahead one step and propose an update for the value of s.\n",
    "\n",
    "  You can assume that the mdp is either infinite or indefinite\n",
    "  horizon (that is, mdp.horizon is inf).\n",
    "\n",
    "  It is possible to handle terminal states either here or in\n",
    "  value iteration. For consistency with our solution, please\n",
    "  handle terminal states in value iteration, not here.\n",
    "\n",
    "  Args:\n",
    "      s: A state.\n",
    "      V: A dict, V[state] -> value.\n",
    "      mdp: An MDP.\n",
    "\n",
    "  Returns:\n",
    "      vs: new value estimate for s.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_bellman_backup():\n",
    "    mdp = SingleRowMDP()\n",
    "    s = 3\n",
    "    V = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    new_V_s = bellman_backup(s, V, mdp)\n",
    "    # Bellman backup should not change V\n",
    "    assert V == {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    assert new_V_s == 0.9 * 10 + 0.1 * -1\n",
    "    s = 2\n",
    "    new_V_s = bellman_backup(s, V, mdp)\n",
    "    assert new_V_s == -1.\n",
    "\n",
    "test1_bellman_backup()\n",
    "def test2_bellman_backup():\n",
    "    mdp = ZitsMDP()\n",
    "    V = {s : 0 for s in mdp.state_space}\n",
    "    assert bellman_backup(0, V, mdp) == -0.4\n",
    "    assert bellman_backup(1, V, mdp) == -0.8\n",
    "    assert bellman_backup(2, V, mdp) == -1.8\n",
    "    assert bellman_backup(3, V, mdp) == -1.8\n",
    "    assert bellman_backup(4, V, mdp) == -1.8\n",
    "\n",
    "test2_bellman_backup()\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's Value in that Iteration\n",
    "Complete the implementation of value iteration for an infinite or indefinite horizon MDP.\n",
    "\n",
    "For reference, our solution is **19** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, max_num_iters=1000, change_threshold=0.0001):\n",
    "  \"\"\"Run value iteration for a certain number of iterations or until\n",
    "  the max change between iterations is below a threshold.\n",
    "\n",
    "  Specifically, you should terminate when:\n",
    "      (max_{s} |V(s) - V'(s)|) < change_threshold\n",
    "  where V is the old value function estimate, V' is the new one,\n",
    "  and |*| denotes absolute value.\n",
    "\n",
    "  You can assume that the mdp is either infinite or indefinite\n",
    "  horizon (that is, mdp.horizon is inf).\n",
    "\n",
    "  Make sure to handle terminal states!\n",
    "\n",
    "  Args:\n",
    "      mdp: An MDP.\n",
    "      max_num_iters: An int representing the maximum number of\n",
    "          iterations to run value iteration before giving up.\n",
    "      change_threshold: A float used to determine when value iteration\n",
    "          has converged and it is safe to terminate.\n",
    "\n",
    "  Returns: \n",
    "      V:  A dict, V[state] -> value.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_value_iteration():\n",
    "    mdp = SingleRowMDP()\n",
    "    V = value_iteration(mdp)\n",
    "    expected_V = {0: 0.0, 1: 5.58531, 2: 8.31706, 3: 9.73170, 4: 0.0}\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "\n",
    "test1_value_iteration()\n",
    "def test2_value_iteration():\n",
    "    mdp = ZitsMDP()\n",
    "    V = value_iteration(mdp)\n",
    "    expected_V = {0: -6.40530, 1: -7.07368, 2: -7.81918, 3: -7.81918, 4: -7.81918}\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "\n",
    "test2_value_iteration()\n",
    "def test3_value_iteration():\n",
    "    mdp = SingleRowMDP()\n",
    "    expected_V = {0: 0.0, 1: -1.9, 2: -1.0, 3: 8.9, 4: 0.0}\n",
    "    V = value_iteration(mdp, max_num_iters=1)\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "    V = value_iteration(mdp, change_threshold=float(\"inf\"))\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "\n",
    "test3_value_iteration()\n",
    "def test4_value_iteration():\n",
    "    mdp = ChaseMDP()\n",
    "    V = value_iteration(mdp)\n",
    "    partial_expected_V = {((0, 1), (0, 1)): 0.0, ((0, 1), (1, 0)): 0.87506,\n",
    "                          ((1, 0), (0, 2)): 0.80601, ((0, 2), (1, 2)): 0.96536,\n",
    "                          ((1, 1), (0, 1)): 0.94896}\n",
    "    for s in partial_expected_V:\n",
    "        assert abs(V[s] - partial_expected_V[s]) < 1e-4\n",
    "\n",
    "test4_value_iteration()\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectimax Search\n",
    "Complete the implementation of expectimax search for a finite horizon MDP.\n",
    "\n",
    "For reference, our solution is **15** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectimax_search(initial_state, mdp, horizon):\n",
    "  \"\"\"Use expectimax search to determine a next action.\n",
    "\n",
    "  Note that we're just computing the single next action to\n",
    "  take, we do not need to store the entire partial V.\n",
    "\n",
    "  Horizon is given as a separate argument so that we can use\n",
    "  expectimax search with receding horizon control, for example,\n",
    "  even if mdp.horizon is inf.\n",
    "\n",
    "  Args:\n",
    "      initial_state: A state in the mdp.\n",
    "      mdp: An MDP.\n",
    "      horizon: An int horizon.\n",
    "\n",
    "  Returns:\n",
    "      action: An action in the mdp.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_expectimax_search():\n",
    "    mdp = MarshmallowMDP()\n",
    "    assert expectimax_search((0, True), mdp, mdp.horizon) == \"wait\"\n",
    "    assert expectimax_search((0, True), mdp, 1) == \"eat\"\n",
    "    assert expectimax_search((3, True), mdp, mdp.horizon) == \"eat\"\n",
    "    assert expectimax_search((1, True), mdp, mdp.horizon) == \"eat\"\n",
    "    assert expectimax_search((2, True), mdp, mdp.horizon) == \"eat\"\n",
    "    assert expectimax_search((1, True), mdp, 10) == \"wait\"\n",
    "\n",
    "test1_expectimax_search()\n",
    "def test2_expectimax_search():\n",
    "    mdp = ChaseMDP()\n",
    "    assert expectimax_search(((0, 0), (0, 1)), mdp, 1) == \"right\"\n",
    "    assert expectimax_search(((0, 0), (0, 2)), mdp, 2) == \"right\"\n",
    "    assert expectimax_search(((0, 0), (1, 0)), mdp, 1) == \"down\"\n",
    "    assert expectimax_search(((0, 0), (1, 2)), mdp, 2) in [\"right\", \"down\"]\n",
    "    assert expectimax_search(((1, 2), (0, 0)), mdp, 2) in [\"up\", \"left\"]\n",
    "\n",
    "test2_expectimax_search()\n",
    "print('Tests passed.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
