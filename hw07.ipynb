{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def state_space(self):\n",
    "        \"\"\"Representation of the MDP state set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def action_space(self):\n",
    "        \"\"\"Representation of the MDP action set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        \"\"\"Gamma, defaults to 1.\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        \"\"\"H, defaults to inf.\n",
    "        \"\"\"\n",
    "        return float(\"inf\")\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        \"\"\"Designate certain states as terminal (done) states.\n",
    "\n",
    "        Defaults to False.\n",
    "\n",
    "        Args:\n",
    "            state: A state.\n",
    "\n",
    "        Returns:\n",
    "            is_terminal : A bool.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Return (deterministic) reward for executing action\n",
    "        in state.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "            next_state: A next state.\n",
    "\n",
    "        Returns:\n",
    "            reward : Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        \"\"\"Return a distribution over next states.\n",
    "\n",
    "        The form of this distribution will vary, e.g., depending\n",
    "        on whether the MDP has discrete or continuous states.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "\n",
    "        Returns:\n",
    "            next_state_distribution: Distribution over next states.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def sample_next_state(self, state, action, rng=np.random):\n",
    "        \"\"\"Sample a next state from the transition distribution.\n",
    "\n",
    "        This function may be overwritten by subclasses when the explicit\n",
    "        distribution is too large to enumerate.\n",
    "\n",
    "        Args:\n",
    "            state: A state from the state space.\n",
    "            action: An action from the action space.\n",
    "            rng: A random number generator.\n",
    "\n",
    "        Returns:\n",
    "            next_state: A sampled next state from the state space.\n",
    "        \"\"\"\n",
    "        next_state_dist = self.get_transition_distribution(state, action)\n",
    "        next_states, probs = zip(*next_state_dist.items())\n",
    "        next_state_index = rng.choice(len(next_states), p=probs)\n",
    "        next_state = next_states[next_state_index]\n",
    "        return next_state\n",
    "\n",
    "\n",
    "class SingleRowMDP(MDP):\n",
    "    \"\"\"A 1D grid MDP for debugging. The grid is 1x5\n",
    "    and the agent is meant to start off in the middle.\n",
    "    There is +10 reward on the rightmost square, -10 on\n",
    "    the left. Actions are left and right. An action effect\n",
    "    is reversed with 10% probability.\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return {0, 1, 2, 3, 4}  # position in grid\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {0, 1}  # left, right\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Discrete distributions, represented with a dict\n",
    "        # mapping next states to probs.\n",
    "        delta = 1 if action == 1 else -1\n",
    "        intended_effect = min(max(state + delta, 0), 4)\n",
    "        opposite_effect = min(max(state - delta, 0), 4)\n",
    "        assert (intended_effect != opposite_effect)\n",
    "        return {intended_effect: 0.9, opposite_effect: 0.1}\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if next_state == 0:\n",
    "          return -10\n",
    "        if next_state == 4:\n",
    "          return 10\n",
    "        return -1  # living penalty\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        return state in {0, 4}\n",
    "\n",
    "\n",
    "class ZitsMDP(MDP):\n",
    "    \"\"\"The Zits MDP described in lecture.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return {0, 1, 2, 3, 4}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {\"apply\", \"sleep\"}\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return 0.9\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if action == \"apply\":\n",
    "            return -1 - next_state\n",
    "        assert action == \"sleep\"\n",
    "        return -next_state\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        if action == \"apply\":\n",
    "            return {\n",
    "                0: 0.8,\n",
    "                4: 0.2\n",
    "            }\n",
    "        assert action == \"sleep\"\n",
    "        return {\n",
    "            min(state + 1, 4): 0.4,\n",
    "            max(state - 1, 0): 0.6\n",
    "        }\n",
    "\n",
    "\n",
    "class ChaseMDP(MDP):\n",
    "    \"\"\"A 2D grid bunny chasing MDP.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "        return np.zeros((2, 3))  # by default, 2x3 grid with no obstacles\n",
    "\n",
    "    @property\n",
    "    def goal_reward(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def living_reward(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "        return self.obstacles.shape[0]\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "        return self.obstacles.shape[1]\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        pos = [(r, c) for r in range(self.height) for c in range(self.width)]\n",
    "        return {(p1, p2) for p1 in pos for p2 in pos}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {'up', 'down', 'left', 'right'}\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return 0.9\n",
    "\n",
    "    def action_to_delta(self, action):\n",
    "        return {\n",
    "            'up': (-1, 0),  # up,\n",
    "            'down': (1, 0),  # down,\n",
    "            'left': (0, -1),  # left,\n",
    "            'right': (0, 1),  # right,\n",
    "        }[action]\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Discrete distributions, represented with a dict\n",
    "        # mapping next states to probs.\n",
    "        next_state_dist = defaultdict(float)\n",
    "\n",
    "        agent_pos, goal_pos = state\n",
    "\n",
    "        # Get next agent state\n",
    "        row, col = agent_pos\n",
    "        dr, dc = self.action_to_delta(action)\n",
    "        r, c = row + dr, col + dc\n",
    "        # Stay in place if out of bounds or obstacle\n",
    "        if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "            r, c = row, col\n",
    "        elif self.obstacles[r, c]:\n",
    "            r, c = row, col\n",
    "        next_agent_pos = (r, c)\n",
    "\n",
    "        # Get next bunny state\n",
    "        # Stay in same place with probability 0.5\n",
    "        next_state_dist[(next_agent_pos, goal_pos)] += 0.5\n",
    "        # Otherwise move\n",
    "        row, col = goal_pos\n",
    "        for (dr, dc) in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            r, c = row + dr, col + dc\n",
    "            # Stay in place if out of bounds or obstacle\n",
    "            if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "                r, c = row, col\n",
    "            elif self.obstacles[r, c]:\n",
    "                r, c = row, col\n",
    "            next_goal_pos = (r, c)\n",
    "            next_state_dist[(next_agent_pos, next_goal_pos)] += 0.5*0.25\n",
    "\n",
    "        return next_state_dist\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        agent_pos, goal_pos = next_state\n",
    "        if agent_pos == goal_pos:\n",
    "            return self.goal_reward\n",
    "        return self.living_reward\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        agent_pos, goal_pos = state\n",
    "        return agent_pos == goal_pos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait, Bellman, Backup!\n",
    "Complete the implementation of the bellman backup for an infinite or indefinite horizon MDP.\n",
    "\n",
    "For reference, our solution is **9** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_backup(s, V, mdp):\n",
    "  \"\"\"Look ahead one step and propose an update for the value of s.\n",
    "\n",
    "  You can assume that the mdp is either infinite or indefinite\n",
    "  horizon (that is, mdp.H is inf).\n",
    "\n",
    "  Args:\n",
    "      s: A state.\n",
    "      V: A dict, V[state] -> value.\n",
    "      mdp: An MDP.\n",
    "\n",
    "  Returns:\n",
    "      vs: new value estimate for s.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_bellman_backup():\n",
    "    mdp = SingleRowMDP()\n",
    "    s = 3\n",
    "    V = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    new_V_s = bellman_backup(s, V, mdp)\n",
    "    # Bellman backup should not change V\n",
    "    assert V == {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    assert new_V_s == 0.9 * 10 + 0.1 * -1\n",
    "    s = 2\n",
    "    new_V_s = bellman_backup(s, V, mdp)\n",
    "    assert new_V_s == -1.\n",
    "\n",
    "test1_bellman_backup()\n",
    "def test2_bellman_backup():\n",
    "    mdp = ZitsMDP()\n",
    "    V = {s : 0 for s in mdp.state_space}\n",
    "    assert bellman_backup(0, V, mdp) == -0.4\n",
    "    assert bellman_backup(1, V, mdp) == -0.8\n",
    "    assert bellman_backup(2, V, mdp) == -1.8\n",
    "    assert bellman_backup(3, V, mdp) == -1.8\n",
    "    assert bellman_backup(4, V, mdp) == -1.8\n",
    "\n",
    "test2_bellman_backup()\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's Value in that Iteration\n",
    "Complete the implementation of value iteration for an infinite or indefinite horizon MDP.\n",
    "\n",
    "For reference, our solution is **19** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, max_num_iters=1000, change_threshold=0.0001):\n",
    "  \"\"\"Run value iteration for a certain number of iterations or until\n",
    "  the max change between iterations is below a threshold.\n",
    "\n",
    "  You can assume that the mdp is either infinite or indefinite\n",
    "  horizon (that is, mdp.H is inf).\n",
    "\n",
    "  Args:\n",
    "      mdp: An MDP.\n",
    "      max_num_iters: An int representing the maximum number of\n",
    "          iterations to run value iteration before giving up.\n",
    "      change_threshold: A float used to determine when value iteration\n",
    "          has converged and it is safe to terminate.\n",
    "\n",
    "  Returns: \n",
    "      V:  A dict, V[state] -> value.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_value_iteration():\n",
    "    mdp = SingleRowMDP()\n",
    "    V = value_iteration(mdp)\n",
    "    expected_V = {0: 0.0, 1: 5.58531, 2: 8.31706, 3: 9.73170, 4: 0.0}\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "\n",
    "test1_value_iteration()\n",
    "def test2_value_iteration():\n",
    "    mdp = ZitsMDP()\n",
    "    V = value_iteration(mdp)\n",
    "    expected_V = {0: -6.40530, 1: -7.07368, 2: -7.81918, 3: -7.81918, 4: -7.81918}\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "\n",
    "test2_value_iteration()\n",
    "def test3_value_iteration():\n",
    "    mdp = SingleRowMDP()\n",
    "    expected_V = {0: 0.0, 1: -1.9, 2: -1.0, 3: 8.9, 4: 0.0}\n",
    "    V = value_iteration(mdp, max_num_iters=1)\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "    V = value_iteration(mdp, change_threshold=float(\"inf\"))\n",
    "    for s in mdp.state_space:\n",
    "        assert abs(V[s] - expected_V[s]) < 1e-4\n",
    "\n",
    "test3_value_iteration()\n",
    "print('Tests passed.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
