{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import abc\n",
    "import numpy as np\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "def press(event):\n",
    "  \"\"\"matplotlib helper function. It processes the keyboard event. If the user hits q, it exit the program.\"\"\"\n",
    "  import matplotlib.pyplot as plt\n",
    "  if event.key == 'q':\n",
    "    sys.exit(0)\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "def plot_value_function(V, mdp):\n",
    "  \"\"\"matplotlib helper function. It takes a value function assumed to be a\n",
    "  maze and plots it.\n",
    "\n",
    "  Args:\n",
    "    V: a dict of {(r, c) : value}\n",
    "  \"\"\"\n",
    "\n",
    "  import matplotlib.pyplot as plt\n",
    "  image = np.zeros((mdp.height, mdp.width))\n",
    "  for state in V.keys():\n",
    "    image[state[0], state[1]] = V[state]\n",
    "\n",
    "  cmap = plt.cm.binary\n",
    "  norm = plt.Normalize(min(V.values()), max(V.values()))\n",
    "  rgba = cmap(norm(image))\n",
    "\n",
    "  for r in range(mdp.height):\n",
    "    for c in range(mdp.width):\n",
    "      if (mdp.obstacles[r][c]):\n",
    "        rgba[r, c, :3] = 1, 0, 0\n",
    "\n",
    "  if mdp.hazards:\n",
    "    for hazard in mdp.hazards:\n",
    "      rgba[hazard[0], hazard[1], :3] = 0, 0, 1\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  fig.canvas.mpl_connect('key_press_event', press)\n",
    "  ax.imshow(rgba)\n",
    "\n",
    "  #\n",
    "  # Note that the image coordinates are in (row, column) order, but the plt.arrow\n",
    "  # command is (x, y). If the rows are y-coordinates, then these two commands\n",
    "  # use different coordinate conventions. Never mind that image coordinates\n",
    "  # also have the origin in the top left corner, and is left-handed.\n",
    "  #\n",
    "  # Computer graphics has much to answer for.\n",
    "  #\n",
    "\n",
    "  for state in V.keys():\n",
    "    r, c = state\n",
    "    delta = mdp.action_to_delta(mdp.get_action(V, state))\n",
    "    plt.arrow(c, r, delta[1]*.35, delta[0]*.35, width = 0.05)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def check_value_function(sub, sol):\n",
    "  assert set(sub) == set(sol), 'Sets of entries do not match.'\n",
    "  for k, v in sol.items():\n",
    "    assert abs(sub[k] - v) < 1e-5, f'Value do not match for {k}: expect {v}, got {sub[k]}'\n",
    "  return True\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def state_space(self):\n",
    "        \"\"\"Representation of the MDP state set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def action_space(self):\n",
    "        \"\"\"Representation of the MDP action set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        \"\"\"Gamma, defaults to 1.\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        \"\"\"H, defaults to inf.\n",
    "        \"\"\"\n",
    "        return float(\"inf\")\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        \"\"\"Designate certain states as terminal (done) states.\n",
    "\n",
    "        Defaults to False.\n",
    "\n",
    "        Args:\n",
    "            state: A state.\n",
    "\n",
    "        Returns:\n",
    "            is_terminal : A bool.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Return (deterministic) reward for executing action\n",
    "        in state.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "            next_state: A next state.\n",
    "\n",
    "        Returns:\n",
    "            reward : Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        \"\"\"Return a distribution over next states.\n",
    "\n",
    "        The form of this distribution will vary, e.g., depending\n",
    "        on whether the MDP has discrete or continuous states.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "\n",
    "        Returns:\n",
    "            next_state_distribution: Distribution over next states.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def sample_next_state(self, state, action, rng=np.random):\n",
    "        \"\"\"Sample a next state from the transition distribution.\n",
    "\n",
    "        This function may be overwritten by subclasses when the explicit\n",
    "        distribution is too large to enumerate.\n",
    "\n",
    "        Args:\n",
    "            state: A state from the state space.\n",
    "            action: An action from the action space.\n",
    "            rng: A random number generator.\n",
    "\n",
    "        Returns:\n",
    "            next_state: A sampled next state from the state space.\n",
    "        \"\"\"\n",
    "        next_state_dist = self.get_transition_distribution(state, action)\n",
    "        next_states, probs = zip(*next_state_dist.items())\n",
    "        next_state_index = rng.choice(len(next_states), p=probs)\n",
    "        next_state = next_states[next_state_index]\n",
    "        return next_state\n",
    "\n",
    "class RescueMDP(MDP):\n",
    "    \"\"\"A 2D grid Rescue MDP. We know where the person to be rescued is, and we\n",
    "    have to go get them.\"\"\"\n",
    "\n",
    "    _goal_location = (4, 5)\n",
    "    goal_reward = 100\n",
    "    living_reward = 0\n",
    "    hazard_cost = -100\n",
    "    hazards = set()\n",
    "    temporal_discount_factor = .9\n",
    "    goal_is_terminal = True\n",
    "\n",
    "    \"\"\"This is the probability that the action has the intended effect. It\n",
    "    needs to be <= 1. If it is less than 1, the rest of the probability mass\n",
    "    is distributed among the other 3 actions. \"\"\"\n",
    "\n",
    "    _correct_transition_probability = .97\n",
    "    _noise_transition_probability = .01\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "      return np.zeros((2, 3))  # by default, 2x3 grid with no obstacles\n",
    "\n",
    "    @property\n",
    "    def goal_location(self):\n",
    "      return self._goal_location\n",
    "\n",
    "    @goal_location.setter\n",
    "    def goal_location(self, location):\n",
    "      assert location[0] < self.width\n",
    "      assert location[1] < self.height\n",
    "      assert not self.obstacles[location[0]][location[1]]\n",
    "      self._goal_location = location\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "      return self.obstacles.shape[0]\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "      return self.obstacles.shape[1]\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "      return {(r, c) for r in range(self.height) for c in range(self.width)\n",
    "              if not self.obstacles[r][c]}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "      return {'up', 'down', 'left', 'right'}\n",
    "\n",
    "    def action_to_delta(self, action):\n",
    "      return {\n",
    "        'up': (-1, 0),  # up,\n",
    "        'down': (1, 0),  # down,\n",
    "        'left': (0, -1),  # left,\n",
    "        'right': (0, 1),  # right,\n",
    "      }[action]\n",
    "\n",
    "    def get_action(self, V, s):\n",
    "\n",
    "      best_value = -float(\"inf\")\n",
    "      best_action = False\n",
    "\n",
    "      for a in self.action_space:\n",
    "        qsa = 0.\n",
    "        for ns, p in self.get_transition_distribution(s, a).items():\n",
    "          r = self.get_reward(s, a, ns)\n",
    "          qsa += p * (r + self.temporal_discount_factor * V[ns])\n",
    "        if qsa > best_value:\n",
    "          best_value = qsa\n",
    "          best_action = a\n",
    "\n",
    "      return best_action\n",
    "\n",
    "\n",
    "    @property\n",
    "    def correct_transition_probability(self):\n",
    "      return self._correct_transition_probability\n",
    "\n",
    "    @correct_transition_probability.setter\n",
    "    def correct_transition_probability(self, prob):\n",
    "      assert prob >= 0 and prob <= 1\n",
    "\n",
    "      # This setter function allows the user to specify a probability of an\n",
    "      # action having the intended effect, e.g., the likelihood that the 'up'\n",
    "      # action moves the agent 'up'. The probability has to be between 0 and\n",
    "      # 1. If it is less than 1, all the remaining probability mass is equally\n",
    "      # distributed among the other three directions.\n",
    "\n",
    "      self._correct_transition_probability = prob\n",
    "      self._noise_transition_probability = (1.0 - prob) / 3.0\n",
    "\n",
    "    @property\n",
    "    def noise_transition_probability(self):\n",
    "      return self._noise_transition_probability\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "      # Discrete distributions, represented with a dict\n",
    "      # mapping next states to probs.\n",
    "      next_state_dist = defaultdict(float)\n",
    "\n",
    "      if self.state_is_terminal(state):\n",
    "        return {state: 1.0}\n",
    "\n",
    "      row, col = state\n",
    "      for (dr, dc) in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "        r, c = row + dr, col + dc\n",
    "        # Stay in place if out of bounds or obstacle\n",
    "        if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "          r, c = row, col\n",
    "        elif self.obstacles[r, c]:\n",
    "          r, c = row, col\n",
    "        next_agent_pos = (r, c)\n",
    "        if self.action_to_delta(action) == (dr, dc):\n",
    "          next_state_dist[next_agent_pos] += self.correct_transition_probability\n",
    "        else:\n",
    "          next_state_dist[next_agent_pos] += self.noise_transition_probability\n",
    "\n",
    "      return next_state_dist\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "      agent_pos = next_state\n",
    "      if agent_pos == self.goal_location:\n",
    "        return self.goal_reward\n",
    "      if self.hazards and next_state in self.hazards:\n",
    "        return self.hazard_cost\n",
    "      return self.living_reward\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "      if not self.goal_is_terminal:\n",
    "        return False\n",
    "      return state == self.goal_location\n",
    "\n",
    "def bellman_backup(s, V, mdp):\n",
    "  \"\"\"Look ahead one step and propose an update for the value of s.\n",
    "\n",
    "  You can assume that the mdp is either infinite or indefinite\n",
    "  horizon (that is, mdp.horizon is inf).\n",
    "\n",
    "  Args:\n",
    "    s: A state.\n",
    "    V: A dict, V[state] -> value.\n",
    "    mdp: An MDP.\n",
    "\n",
    "    Returns:\n",
    "      vs: new value estimate for s.\n",
    "  \"\"\"\n",
    "\n",
    "  assert mdp.horizon == float(\"inf\")\n",
    "  vs = -float(\"inf\")\n",
    "  for a in mdp.action_space:\n",
    "    qsa = 0.\n",
    "    for ns, p in mdp.get_transition_distribution(s, a).items():\n",
    "      r = mdp.get_reward(s, a, ns)\n",
    "      qsa += p * (r + mdp.temporal_discount_factor * V[ns])\n",
    "    vs = max(qsa, vs)\n",
    "  return vs\n",
    "\n",
    "def value_iteration(mdp, max_num_iters=1000, change_threshold=1e-4):\n",
    "  \"\"\"Run value iteration for a certain number of iterations or until\n",
    "  the max change between iterations is below a threshold.\n",
    "\n",
    "  You can assume that the mdp is either infinite or indefinite\n",
    "  horizon (that is, mdp.horizon is inf).\n",
    "\n",
    "  Args:\n",
    "    mdp: An MDP.\n",
    "    max_num_iters: An int representing the maximum number of\n",
    "    iterations to run value iteration before giving up.\n",
    "    change_threshold: A float used to determine when value iteration\n",
    "    has converged and it is safe to terminate.\n",
    "\n",
    "  Returns:\n",
    "    V:  A dict, V[state] -> value.\n",
    "    it: The number of iterations before convergence.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize V to all zeros\n",
    "  V = {s: 0. for s in mdp.state_space}\n",
    "\n",
    "  for it in range(max_num_iters):\n",
    "    next_V = {}\n",
    "    max_change = 0.\n",
    "    for s in mdp.state_space:\n",
    "      if mdp.state_is_terminal(s):\n",
    "        next_V[s] = 0.\n",
    "      else:\n",
    "        next_V[s] = bellman_backup(s, V, mdp)\n",
    "      max_change = max(abs(next_V[s] - V[s]), max_change)\n",
    "\n",
    "    V = next_V\n",
    "\n",
    "    if max_change < change_threshold:\n",
    "      break\n",
    "\n",
    "  return V, it\n",
    "\n",
    "\n",
    "class SmallRescueMDP(RescueMDP):\n",
    "  \"\"\"A small 2D grid MDP.\"\"\"\n",
    "\n",
    "  goal_location = (0, 0)\n",
    "  @property\n",
    "  def obstacles(self):\n",
    "    return np.array([\n",
    "      [0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0],\n",
    "      [0, 0, 0, 1, 0],\n",
    "      [0, 1, 0, 1, 1]\n",
    "      ])\n",
    "\n",
    "class LargeRescueMDP(RescueMDP):\n",
    "  \"\"\"A larger 2D grid MDP.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def obstacles(self):\n",
    "    return np.array([\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "      [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "      [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup 1\n",
    "\n",
    "We have provided a new RescueMDP class for you, that models the robot's motion\n",
    "as noisy. This class is in many ways similar to the ChaseMDP class in homework\n",
    "7, in that it is a maze MDP with obstacles. However, there's just the one\n",
    "agent (the robot, no bunny).\n",
    "\n",
    "Recall that an MDP is defined by the following tuple:\n",
    "* States: The state space is a set of coordinates. The set is defined over a\n",
    "grid, but states that aren't obstacles aren't in the state space.\n",
    "* Actions: The robot can take four actions, up, down, left, right.\n",
    "* A transition function: The probability the action succeeds at moving the\n",
    "robot in the given direction is given by the class field\n",
    "`correct_transition_probability`. If this probability is less than 1, then the\n",
    "rest of the probability mass is uniformly distributed among the other three\n",
    "directions. Any probability mass for a motion that would move the robot into\n",
    "an obstacle or out of the map is mapped to the robot staying the same\n",
    "place. If the robot is in the goal state and the MDP has the flag\n",
    "`goal_is_terminal = True`, it cannot transition out of the goal state. If the flag\n",
    "`goal_is_terminal = False`, the robot is free to leave the goal state and\n",
    "re-enter it.\n",
    "* Reward function: The robot gets a reward of `living_reward` for each action\n",
    "it takes. It gets reward of `goal_reward` every time it enters the goal\n",
    "state.\n",
    "\n",
    "Let's begin solving a small version of the RescueMDP. The maze is 4x5 ---\n",
    "remember that arrays are row-major order, that is, the arrays are indexed by\n",
    "(row, column). The person to be rescued (goal_location) is at (0, 0).\n",
    "\n",
    "In the first warmup problem, please solve\n",
    "for the optimal value function for this problem. We have provided the reference\n",
    "implementation for value iteration.\n",
    "\n",
    "We also want you to be able to examine the policy that results from solving\n",
    "for the optimal value function. Please use the helper function\n",
    "`plot_value_function` to generate a plot of both the value function and the\n",
    "corresponding policy.\n",
    "\n",
    "<!-- first_experiment() -->\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 1:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 1.\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **3** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_1():\n",
    "  \"\"\"Creates a SmallRescueMDP() and returns the optimal value function.\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup1_test():\n",
    "  sol = {(0, 1): 99.50024542567529, (1, 2): 80.04025632412159, (3, 2): 64.440383547797, (0, 0): 0.0, (3, 0): 79.94774469404808, (1, 0): 99.50025184467293, (2, 2): 71.82187169836179, (1, 4): 64.17423641835612, (2, 1): 80.04026274311923, (2, 0): 89.1055619783276, (1, 1): 89.19994673349667, (2, 4): 57.578723372930945, (0, 4): 71.59323394182438, (0, 3): 79.87054749786864, (0, 2): 89.10485517953946}\n",
    "  check_value_function(warmup_1(), sol)\n",
    "\n",
    "warmup1_test()\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup 2\n",
    "Please hand-code a list of actions that will get the robot from (3,2) to the goal location (0, 0) under the optimal policy, if `correct_transition_probability = 1.0`. This should be a python list of actions, e.g., [down, down, down, down].\n",
    "\n",
    "For reference, our solution is **2** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup2():\n",
    "  \"\"\"Hand-code a list of actions that the robot will take under the optimal\n",
    "  policy from a start state of (3, 2) to the goal state of (0, 0), if\n",
    "  `correct_transition_probability = 1.0`.\n",
    "\n",
    "  Returns:\n",
    "    actions: A list of str actions that get the robot to the goal state.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup2_test():\n",
    "  mdp = SmallRescueMDP()\n",
    "  mdp.correct_transition_probability = 1.0\n",
    "  state = (3, 2)\n",
    "  actions = warmup2()\n",
    "  for action in actions:\n",
    "    state = mdp.sample_next_state(state, action)\n",
    "  assert state == (0, 0)\n",
    "\n",
    "warmup2_test()\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Question 2\n",
    "\n",
    "In the MDP class we have provided, the goal state is a terminal state. Once\n",
    "the agent is in the terminal state, it can't leave and it doesn't receive any\n",
    "more reward. Let's investigate what happens if we don't make the goal state a\n",
    "terminal state.\n",
    "\n",
    "Please create a SmallRescueMDP, set the goal_is_terminal to be False, and\n",
    "compute the value function.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 2:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 2.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to the previous question, you should have seen the number of\n",
    "iterations change before value iteration converged. Why is that? What extra\n",
    "work are we making value iteration do in this case?\n",
    "\n",
    "* At the same time, the optimal policy did not change anywhere except possibly\n",
    "the goal. (In this and the following questions, when we ask you to compare\n",
    "policies, we want you to ignore the goal state.) Why is the policy the same,\n",
    "even though it took more iterations to converge?\n",
    "\n",
    "* Is there some property of the reward function in general for MDPs that makes\n",
    "solving for the optimal policy with value iteration easier or harder?\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **5** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_2():\n",
    "  \"\"\"Creates a SmallRescueMDP(), sets goal_is_terminal to False and returns the optimal value function.\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V, rc):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub, rc_sub = function()\n",
    "  assert rc == rc_sub\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_2, {(0, 1): 976.6314991080653, (1, 2): 785.624375483249, (3, 2): 632.5057580306595, (0, 0): 979.4854087102677, (3, 0): 784.7163389217253, (1, 0): 976.631562112438, (2, 2): 704.957837213064, (1, 4): 629.8934334526643, (2, 1): 785.6244384876218, (2, 0): 874.6037652852367, (1, 1): 875.5301872186959, (2, 4): 565.1560925806813, (0, 4): 702.7136725549406, (0, 3): 783.9586195544681, (0, 2): 874.5968278037411}, 131)\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Question 3\n",
    "\n",
    "Let's now turn our attention to a larger MDP. Please create a LargeRescueMDP\n",
    "and compute the optimal value function.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 3:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 3.\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **3** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_3():\n",
    "  \"\"\"Creates a LargeRescueMDP(), and returns the optimal value function and\n",
    "  number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V, rc):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub, rc_sub = function()\n",
    "  assert rc == rc_sub\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_3, {(5, 9): 51.427184372629654, (4, 7): 57.24942466484467, (6, 9): 57.31044771948743, (7, 3): 64.38378339850985, (4, 8): 51.477861619010575, (3, 0): 29.997978602266883, (0, 2): 51.77167752199026, (2, 8): 63.866697606203566, (5, 6): 89.10343277693175, (6, 6): 79.87591094828778, (7, 7): 64.1906123496321, (0, 7): 57.53968483878395, (2, 1): 37.294652947208995, (8, 0): 41.56749854170138, (8, 9): 46.46228722379235, (1, 6): 71.46317864388475, (3, 7): 63.866705918797976, (0, 3): 57.81353882364696, (2, 5): 89.01900243377555, (8, 5): 71.26442203181726, (5, 8): 57.30901926222007, (4, 0): 26.97240030571446, (1, 2): 46.361654557652294, (4, 9): 46.28196469917531, (5, 5): 99.40514065110256, (2, 9): 57.24850935593198, (6, 7): 71.52909705147924, (8, 1): 46.37295740578368, (7, 6): 71.6037666010765, (4, 4): 99.50187546410399, (6, 3): 71.75171813607103, (1, 5): 79.64569348621423, (5, 0): 30.00062894204498, (2, 2): 41.5179757146076, (0, 4): 64.49822454430208, (8, 6): 64.24856551191186, (4, 1): 29.90300033148703, (1, 1): 41.5626015183776, (6, 4): 79.96208134752649, (3, 2): 37.21529472183023, (0, 0): 41.55773396442767, (2, 6): 79.71607202444552, (5, 4): 89.19904388402188, (8, 2): 51.784353648073704, (7, 1): 41.52806356045833, (4, 5): 0.0, (6, 0): 33.4683494865608, (1, 4): 71.88481819474575, (3, 9): 51.378797571401286, (0, 5): 71.33026555348314, (7, 5): 79.57152694202794, (8, 7): 57.651153477926165, (4, 2): 33.35853220876574, (1, 0): 37.29501402141274, (0, 8): 51.577016904150774, (6, 5): 88.9353734615529, (3, 5): 99.50024037525212, (0, 1): 46.36201563185604, (2, 7): 71.31884930507499, (5, 3): 79.95573398571852, (7, 8): 57.5446502393624, (7, 0): 37.29908558822458, (6, 8): 63.99437878087923, (7, 9): 51.63515062834477, (8, 3): 57.77191141593772, (6, 1): 37.22526933032567, (3, 8): 57.25614447733788, (0, 6): 64.12430323875384, (2, 0): 33.465368707503046, (7, 4): 71.6813569329961, (8, 8): 51.73109931662876, (1, 7): 64.12419955940373, (0, 9): 46.27619295995178, (3, 4): 89.28524333009564, (2, 4): 80.11518850941486, (8, 4): 64.31801372707054}, 22)\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Question 4\n",
    "\n",
    "The default transition model for the LargeRescueMDP is pretty close\n",
    "to deterministic. There is a .97 chance that each action succeeds in the intended\n",
    "direction (unless there's an obstacle in the way or its the edge of the map)\n",
    "and only a .01 chance of ending up in one of the other 3 neighbouring grid\n",
    "cells.\n",
    "\n",
    "What happens if we make the transition model noisier? Setting the\n",
    "correct_transition_probability to .76 is not too noisy, but it has\n",
    "implications for our ability to solve for the optimal policy.\n",
    "\n",
    "Please create a LargeRescueMDP and set the correct_transition_probability to\n",
    "be .76. You can set the correct_transition_probability field of the\n",
    "LargeRescueMDP class directly, and the transition function will be adjusted\n",
    "according. (You can read the comment in the LargeRescueMDP setter function for\n",
    "more detail on how setting the correct_transition_probability works.)\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 4:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 4.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to the previous question, you should have seen the number of\n",
    "iterations change before value iteration converged. Why is that? What extra\n",
    "work are we making value iteration do in this case?\n",
    "\n",
    "<!-- Because the motion to the next state is noisy, it can take more steps to get\n",
    "to the goal. Value iteration converges when the expected reward of the policy\n",
    "from each state to the goal has been computed. Since it takes more steps to\n",
    "get to the goal, it takes more iterations to compute the expected reward. -->\n",
    "\n",
    "* At the same time, the optimal policy did not change. Why didn't the policy\n",
    "change under the noisy dynamics?\n",
    "\n",
    "<!-- There isn't a way for the agent to compensate for the noisy dynamics, so\n",
    "there's no change to the policy that could improve the progress towards the\n",
    "goal. -->\n",
    "\n",
    "* You should have seen the value function decrease in nearly all cells. Why\n",
    "did it go down? Where is the loss in value coming from?\n",
    "\n",
    "<!-- The loss in value at each state comes from the fact that more actions are\n",
    "likely to be required to get to the goal, and so the value of the goal state\n",
    "is likely to be discounted more, and the discounted expected value\n",
    "of a sequence of actions will be reduced. -->\n",
    "\n",
    "* What would happen to the number of iterations, to the policy, and to the\n",
    "value function if the dynamics were even noisier, say\n",
    "correct_transition_probability was set to 0.5?\n",
    "\n",
    "<!-- The number of iterations would go up, the value function would be reduced\n",
    "further, but the policy would not change. -->\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **4** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_4():\n",
    "  \"\"\"Creates a LargeRescueMDP(), sets the correct_transition_probability to\n",
    "  .76 and returns the optimal value function and\n",
    "  number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V, rc):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub, rc_sub = function()\n",
    "  assert rc == rc_sub\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_4, {(5, 9): 37.09563198565326, (4, 7): 41.79958505037646, (6, 9): 42.379150021338276, (7, 3): 53.62383581844224, (4, 8): 37.33611818768401, (3, 0): 19.184941573721872, (0, 2): 39.54500010633741, (2, 8): 48.37609286765003, (5, 6): 80.39156750674351, (6, 6): 69.17350279458515, (7, 7): 51.413594478870216, (0, 7): 44.47885137481426, (2, 1): 25.397822670558156, (8, 0): 29.485520779767217, (8, 9): 34.81853504895561, (1, 6): 58.371006293552576, (3, 7): 48.38047730522361, (0, 3): 46.568742068171915, (2, 5): 79.50785163099987, (8, 5): 57.16960149114739, (5, 8): 42.2743189133091, (4, 0): 17.193656448413645, (1, 2): 33.63013421547188, (4, 9): 33.24527236077266, (5, 5): 93.32556960074426, (2, 9): 41.7430747582296, (6, 7): 58.8166743393827, (8, 1): 34.191129369373066, (7, 6): 59.54114719666043, (4, 4): 94.63963415294967, (6, 3): 61.46654014072949, (1, 5): 67.23721597706378, (5, 0): 19.36445038655641, (2, 2): 28.67493693106001, (0, 4): 54.116379728735176, (8, 6): 51.83291668257821, (4, 1): 18.46056909239355, (1, 1): 29.10269089376744, (6, 4): 70.24275282935724, (3, 2): 24.704121310787116, (0, 0): 29.033502757353006, (2, 6): 67.54061291729131, (5, 4): 81.57596106315178, (8, 2): 40.21565223450838, (7, 1): 29.15147584428534, (4, 5): 0.0, (6, 0): 22.424055347153732, (1, 4): 62.444310480187774, (3, 9): 36.70592766228915, (0, 5): 57.71031630612815, (7, 5): 66.56559260006958, (8, 7): 45.25820887982636, (4, 2): 21.29292539099053, (1, 0): 25.425642667196094, (0, 8): 38.3562257828573, (6, 5): 78.66538264988357, (3, 5): 94.47917738184584, (0, 1): 33.657954212109814, (2, 7): 56.82786088085701, (5, 3): 70.35461243079723, (7, 8): 44.3619253645486, (7, 0): 25.734404278265206, (6, 8): 49.519712872964426, (7, 9): 38.68727231509817, (8, 3): 46.729294692291475, (6, 1): 25.180061683772845, (3, 8): 41.906350224012456, (0, 6): 50.95192651060614, (2, 0): 22.19943115828667, (7, 4): 60.490236730134086, (8, 8): 39.50173149602546, (1, 7): 50.88346010072386, (0, 9): 33.46384586176194, (3, 4): 82.66821248865962, (2, 4): 71.94570365595851, (8, 4): 52.64659733120905}, 41)\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Question 5\n",
    "\n",
    "The default temporal_discount_factor is .9. This is in general quite a low\n",
    "discount factor. An action 20 steps away will have $.9^{20} \\approx .12$ impact on later\n",
    "actions.\n",
    "\n",
    "What happens if we increase the discount factor? Please create a\n",
    "LargeRescueMDP, set the temporal_discount_factor to be .99, and also set\n",
    "the correct_transition_probability to be .76. Then please solve for the\n",
    "optimal value function.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 5:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 5.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to the previous question, you should have seen the number of\n",
    "iterations increase before value iteration converged. Why is that? What\n",
    "extra work are we making value iteration do in this case?\n",
    "\n",
    "<!-- This is a subtle question. If the value of a state is the expected return\n",
    "of a discounted sequence of rewards, the value function converges when we've\n",
    "done enough backups that the effect of one more backup and one more discounted\n",
    "reward is no longer measurable numerically. When we make the value function\n",
    "close to one, then it takes a longer sequence of backups for the discounted\n",
    "reward to no longer be measurable numerically. -->\n",
    "\n",
    "* At the same time, the optimal policy did not change. Why is the policy the\n",
    "same, but it took more iterations to converge?\n",
    "\n",
    "<!-- There's no reason to do anything different, we're just making the value\n",
    "function compute the expected value of a longer sequence of possible outcomes\n",
    "under the policy. -->\n",
    "\n",
    "* You should have seen the value function increase in nearly all cells. Why\n",
    "did it go up? Where did the increase in value come from?\n",
    "\n",
    "<!-- The increase in value came from the fact that we're not down-weighting\n",
    "the future as much. For the same sequence of actions, we expect to get a\n",
    "higher reward because we value the future more. -->\n",
    "\n",
    "* What would happen to the number of iterations, to the policy, and to the\n",
    "value function if we left the temporal_discount_factor at .99 but made the\n",
    "dynamics fairly deterministic again, say correct_transition_probability = .99?\n",
    "(You should try this!)\n",
    "\n",
    "<!-- The number of iterations will go down because the path is nearly\n",
    "deterministic now --- from each state, the policy will get to the goal in\n",
    "(mostly) a fixed number of steps. The fact that we can take expectations over\n",
    "longer sequences of actions doesn't change the value function or make us\n",
    "compute more iterations because once we get to the goal, we get no more\n",
    "reward. The policy won't change, but the value function will stay high. -->\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **5** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_5():\n",
    "  \"\"\"Creates a LargeRescueMDP(), sets the correct_transition_probability to\n",
    "  .76 and the temporal_discount_factor to .99 and returns the optimal value function and\n",
    "  number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V, rc):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub, rc_sub = function()\n",
    "  assert rc == rc_sub\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_5, {(5, 9): 90.49294151104122, (4, 7): 91.53586068943369, (6, 9): 91.67470906173104, (7, 3): 93.97491750094179, (4, 8): 90.54408402590663, (3, 0): 84.8709760689106, (0, 2): 91.15195158169959, (2, 8): 92.85540170289251, (5, 6): 97.7688940677295, (6, 6): 96.30174912325705, (7, 7): 93.49470937074302, (0, 7): 92.17514466317554, (2, 1): 87.23080746083865, (8, 0): 88.55907148092525, (8, 9): 90.02820552034618, (1, 6): 94.66146525243018, (3, 7): 92.85684581202082, (0, 3): 92.67280996413947, (2, 5): 97.64083406000782, (8, 5): 94.47412332570079, (5, 8): 91.64279788012988, (4, 0): 84.0088571825647, (1, 2): 89.67672656160515, (4, 9): 89.54219638658633, (5, 5): 99.22275748726061, (2, 9): 91.51907133407133, (6, 7): 94.7388560346764, (8, 1): 89.85966251042251, (7, 6): 94.86949260973816, (4, 4): 99.39679280725164, (6, 3): 95.24349019068275, (1, 5): 96.0088117943946, (5, 0): 84.979923830497, (2, 2): 88.2596929505786, (0, 4): 94.06459700794483, (8, 6): 93.58873399735727, (4, 1): 84.52926790610293, (1, 1): 88.40199915122935, (6, 4): 96.47708368340508, (3, 2): 86.97078685302058, (0, 0): 88.39225620136497, (2, 6): 96.04265401195448, (5, 4): 97.92940667071218, (8, 2): 91.34026747888913, (7, 1): 88.4404389970416, (4, 5): 0.0, (6, 0): 86.21161978985722, (1, 4): 95.40750777342734, (3, 9): 90.3793968171442, (0, 5): 94.57557687282643, (7, 5): 95.897070647761, (8, 7): 92.35290042089164, (4, 2): 85.70758382182571, (1, 0): 87.24220975351437, (0, 8): 90.84234985893639, (6, 5): 97.51833426545349, (3, 5): 99.37197204832822, (0, 1): 89.68812885428089, (2, 7): 94.36773185735558, (5, 3): 96.51303468508517, (7, 8): 92.13682235192404, (7, 0): 87.385190483955, (6, 8): 93.09749758158448, (7, 9): 90.91508992377925, (8, 3): 92.71010661836499, (6, 1): 87.17982482641227, (3, 8): 91.54877634429582, (0, 6): 93.41218842117453, (2, 0): 86.08978494897735, (7, 4): 95.05380837452687, (8, 8): 91.131398346243, (1, 7): 93.39431118581003, (0, 9): 89.65081472852465, (3, 4): 98.08785113514071, (2, 4): 96.75361408485887, (8, 4): 93.77006553865067}, 52)\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Question 6\n",
    "\n",
    "Let's make the problem a little harder. In addition to obstacles, our\n",
    "RescueMDP class supports hazards, which are states that have a penalty for\n",
    "entering.\n",
    "\n",
    "Let's start with relatively noise-free dynamics and add some hazards. You can\n",
    "add hazards to the LargeRescueMDP simply by setting the LargeRescueMDP.hazard\n",
    "field to be a set of states, for example `mdp.hazards = {(0, 0)}` would put a\n",
    "hazard at (0,0. The default cost of entering a hazard is -100, and is given by\n",
    "the hazard_cost field.\n",
    "\n",
    "Please create a LargeRescueMDP and set a hazard at `{(1, 4)}', and then please\n",
    "solve for the optimal value function.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 6:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 6.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to question three (LargeRescueMDP, discount factor = .9,\n",
    "correct_transition_probability = .97, no hazards), the number of iterations to\n",
    "solve for the optimal value function should be more or less the same, and the\n",
    "policy is the same too. Why didn't the hazard change the policy or require\n",
    "more iterations?\n",
    "\n",
    "<!-- The location of the hazard wasn't on the most likely path to the goal for\n",
    "most states, and with nearly-deterministic dynamics, there was rarely a need\n",
    "to react to the hazard. -->\n",
    "\n",
    "* You should have seen the value function decrease a little bit some but not\n",
    "all states. Where did the value function change and why?\n",
    "\n",
    "<!-- The states where the expected trajectory took the agent past the hazard\n",
    "saw a small reduction in value, due to the small chance that the stochastic\n",
    "dynamcs would take the agent into the hazard, e.g. (0, 0). The states where\n",
    "the expected trajectory took the agent nowhere near the hazard saw no change\n",
    "in value because for those states, the problem looks exactly the same as\n",
    "before. -->\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **4** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_6():\n",
    "  \"\"\"Creates a LargeRescueMDP(), and sets a hazard at `{(1, 4)}' and returns\n",
    "  the optimal value function and number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V, rc):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub, rc_sub = function()\n",
    "  assert rc == rc_sub\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_6, {(5, 9): 51.427181234702594, (4, 7): 57.237127427222504, (6, 9): 57.31044726342632, (7, 3): 64.38372836208329, (4, 8): 51.47763537578787, (3, 0): 28.910227101699018, (0, 2): 49.87899851815338, (2, 8): 63.85286640726496, (5, 6): 89.10343214022201, (6, 6): 79.87591037175028, (7, 7): 64.19061187724431, (0, 7): 57.51957673820742, (2, 1): 35.93122809864921, (8, 0): 41.56746301592025, (8, 9): 46.46228687265732, (1, 6): 71.43831962228849, (3, 7): 63.852875619683495, (0, 3): 55.699980287618324, (2, 5): 88.99991706193646, (8, 5): 71.26442007162068, (5, 8): 57.309016754933836, (4, 0): 26.95248971914437, (1, 2): 44.6667562013234, (4, 9): 46.28166592379238, (5, 5): 99.40513994083895, (2, 9): 57.23611303768122, (6, 7): 71.52909653492864, (8, 1): 46.372917773182785, (7, 6): 71.60376607415502, (4, 4): 99.50178865637716, (6, 3): 71.75165679504416, (1, 5): 78.60277616305034, (5, 0): 30.00041951723898, (2, 2): 40.00015345905191, (0, 4): 62.140285975069276, (8, 6): 64.24856502597083, (4, 1): 28.818721112650692, (1, 1): 40.043147830290565, (6, 4): 79.9620129943958, (3, 2): 35.85477104610539, (0, 0): 40.03845822373549, (2, 6): 79.69889313860067, (5, 4): 89.19896685795916, (8, 2): 51.78430939064777, (7, 1): 41.52802806823758, (4, 5): 0.0, (6, 0): 33.46831919941581, (1, 4): 69.93279454298955, (3, 9): 51.36784781407325, (0, 5): 70.38983783260018, (7, 5): 79.57152527766405, (8, 7): 57.65115304200527, (4, 2): 32.13908541869281, (1, 0): 35.931575973215075, (0, 8): 51.55899254670664, (6, 5): 88.93537220614647, (3, 5): 99.49998093875435, (0, 1): 44.66710407588928, (2, 7): 71.30340318704894, (5, 3): 79.95566494720325, (7, 8): 57.54464981569035, (7, 0): 37.29905369502728, (6, 8): 63.994378299447355, (7, 9): 51.635150247789156, (8, 3): 57.77186204125582, (6, 1): 37.225237499941, (3, 8): 57.24384556138352, (0, 6): 64.0936808893598, (2, 0): 32.242015662748486, (7, 4): 71.68129626521419, (8, 8): 51.73109892557692, (1, 7): 64.10197764838084, (0, 9): 46.26002105405622, (3, 4): 89.27576186090349, (2, 4): 79.07985318770223, (8, 4): 64.31795981689132}, 22)\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Question 7\n",
    "\n",
    "Given the hazards, let's make the dynamics noisier. Please create a\n",
    "LargeRescueMDP, set a hazard at `{(1, 4)}' and set\n",
    "correct_transition_probability to be .76. Then please solve for the optimal\n",
    "value function.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 7:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 7.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to the previous question, with these noisier dynamics, the policy\n",
    "changed slightly. Please enumerate which states had a different policy\n",
    "relative to the previous question. Why did the policy change?\n",
    "\n",
    "<!-- The policy changed in states (4,1) and (3,0). For those two states, the\n",
    "risk of encountering the hazard, and the associated penalty, increased enough\n",
    "relative to the last problem that it made more sense to go down and around the\n",
    "red obstacle, rather than up and over. Those two states were right on the\n",
    "borderline of whether the higher value policy was to go up and over or down\n",
    "and under. When the dynamics are deterministic,\n",
    "it's cheaper to go up and over. The increased noise made it cheaper to go down\n",
    "and under and avoid the hazard.\n",
    "\n",
    "-->\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **5** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_7():\n",
    "  \"\"\"Creates a LargeRescueMDP(), and sets a hazard at `{(1, 4)}' and\n",
    "  correct_transition_probability to 0.76 and returns\n",
    "  the optimal value function and number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V, rc):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub, rc_sub = function()\n",
    "  assert rc == rc_sub\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_7, {(5, 9): 37.07537343488191, (4, 7): 40.78746909967961, (6, 9): 42.37407148162809, (7, 3): 53.58554101032665, (4, 8): 37.1634801312472, (3, 0): 14.230813317884076, (0, 2): 23.108408036299814, (2, 8): 47.12293716181593, (5, 6): 80.38707128747696, (6, 6): 69.16929667444792, (7, 7): 51.41006184061299, (0, 7): 42.64943692223374, (2, 1): 14.863986654032782, (8, 0): 29.46475020938977, (8, 9): 34.8158931609956, (1, 6): 56.04226794018405, (3, 7): 47.13202325968897, (0, 3): 27.21267076871142, (2, 5): 77.61195523822383, (8, 5): 57.16014032808957, (5, 8): 42.25607343152651, (4, 0): 16.417279376414367, (1, 2): 19.652639445695943, (4, 9): 33.05063069273245, (5, 5): 93.3203854826171, (2, 9): 40.67035951266604, (6, 7): 58.81295491134314, (8, 1): 34.167086865752225, (7, 6): 59.5370707976482, (4, 4): 94.56343356296915, (6, 3): 61.42204230929323, (1, 5): 55.62362699374977, (5, 0): 19.2819944265021, (2, 2): 16.760919192648494, (0, 4): 31.623160073891142, (8, 6): 51.82890984693313, (4, 1): 14.191084839515755, (1, 1): 17.0099576435439, (6, 4): 70.19547949830189, (3, 2): 14.465702416468297, (0, 0): 16.96817859825391, (2, 6): 65.84768967314606, (5, 4): 81.5163647062676, (8, 2): 40.18738355335795, (7, 1): 29.130901945561014, (4, 5): 0.0, (6, 0): 22.40256585176668, (1, 4): 43.10797405920402, (3, 9): 35.85749615073728, (0, 5): 47.19767018231996, (7, 5): 66.5564719691506, (8, 7): 45.25474917910896, (4, 2): 12.752648917984438, (1, 0): 14.880067322229955, (0, 8): 36.7786361960072, (6, 5): 78.65747352508642, (3, 5): 94.26050523960491, (0, 1): 19.668720113893116, (2, 7): 55.347714169201915, (5, 3): 70.30324824600832, (7, 8): 44.35876223193297, (7, 0): 25.715836239724695, (6, 8): 49.51526218136772, (7, 9): 38.68434291514711, (8, 3): 46.69644778404677, (6, 1): 25.161820498763344, (3, 8): 40.88866472418537, (0, 6): 48.277912397590796, (2, 0): 13.224964116036833, (7, 4): 60.452234839436066, (8, 8): 39.49873186737364, (1, 7): 48.9102433841067, (0, 9): 32.08748163097266, (3, 4): 81.74566789526216, (2, 4): 60.997706610430875, (8, 4): 52.61530487725168}, 43)\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Question 8\n",
    "\n",
    "Now let's make the dynamics really noisy. Once again, please create a\n",
    "LargeRescueMDP with a hazard at `{(1, 4)}', but with\n",
    "correct_transition_probability set to 0.5. You should see a very substantial\n",
    "change in both the policy and value function.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 8:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 8.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to the previous question, with these much noisier dynamics, the\n",
    "policy changed a lot. Where did the policy change (you don't need to\n",
    "enumerate states, just give a general description) and why?\n",
    "\n",
    "<!-- The policy changed in the top left. It's cheaper now for all the states\n",
    "to the left of (0, 4) to go down and around, to avoid the large negative\n",
    "hazard.\n",
    "-->\n",
    "\n",
    "* At the state (0, 3), you should see that the policy from state (0, 3)\n",
    "actually goes to the left, even though it's a much shorter path to the goal\n",
    "to go right. To the nearest 1/100th, what is the lowest value of\n",
    "correct_transition_probability before the agent starts heading left from\n",
    "(0, 3)? (We're expecting you to work this out experimentally, not derive the\n",
    "answer mathematically.)\n",
    "\n",
    "<!-- 0.59. The agent heads right from (0, 3) for values of\n",
    "correct_transition_probability at .59 or greater. .58 or lower, it heads left.\n",
    "-->\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **5** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_8():\n",
    "  \"\"\"Creates a LargeRescueMDP(), and sets a hazard at `{(1, 4)}' and\n",
    "  correct_transition_probability to 0.5 and returns\n",
    "  the optimal value function and number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V, rc):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub, rc_sub = function()\n",
    "  assert rc == rc_sub\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_8, {(5, 9): 14.293773308570756, (4, 7): 12.257729404123905, (6, 9): 17.470929083719025, (7, 3): 31.763122639369513, (4, 8): 13.032557691482545, (3, 0): 2.9670510404427017, (0, 2): -0.20781732009289544, (2, 8): 14.623495028538457, (5, 6): 58.417081668751784, (6, 6): 44.27678096369422, (7, 7): 26.517051201155294, (0, 7): 11.988658922265932, (2, 1): 1.5783110450404056, (8, 0): 10.878573027487132, (8, 9): 15.238821727444563, (1, 6): 17.433410439493713, (3, 7): 14.723408455358632, (0, 3): -4.701281693660509, (2, 5): 41.64410517348415, (8, 5): 30.829638790982713, (5, 8): 16.831639699010523, (4, 0): 3.9085329993652533, (1, 2): 0.9073556276657573, (4, 9): 11.753646222356124, (5, 5): 76.11208979327662, (2, 9): 11.691554230957815, (6, 7): 32.12324354829552, (8, 1): 13.973656429560908, (7, 6): 33.91437634728944, (4, 4): 80.10218352076012, (6, 3): 39.44059244534516, (1, 5): 3.022657227097599, (5, 0): 5.390778213003905, (2, 2): 1.3841713034879826, (0, 4): -21.315413465306385, (8, 6): 27.34061864611354, (4, 1): 3.0091125874058258, (1, 1): 1.1974311585861637, (6, 4): 47.243646625788344, (3, 2): 1.7861164466125632, (0, 0): 1.1610960979976075, (2, 6): 28.582035217246688, (5, 4): 61.773838936434885, (8, 2): 19.299393310263866, (7, 1): 10.407323848359, (4, 5): 0.0, (6, 0): 7.082840364895587, (1, 4): -14.16789428451299, (3, 9): 10.690151502522395, (0, 5): 3.0643671550462237, (7, 5): 39.40783614141224, (8, 7): 22.357412587191533, (4, 2): 2.317116216835473, (1, 0): 1.5389420673207934, (0, 8): 9.345469276586272, (6, 5): 53.88967949297427, (3, 5): 76.54987697070835, (0, 1): 0.8020744337498172, (2, 7): 19.59916846774346, (5, 3): 48.1633084382923, (7, 8): 20.556565474493652, (7, 0): 8.845736779684062, (6, 8): 22.684327610380464, (7, 9): 16.655193698431397, (8, 3): 25.36339749449463, (6, 1): 8.20816411897359, (3, 8): 12.377457524096181, (0, 6): 11.885859730077536, (2, 0): 2.1208429059739875, (7, 4): 36.3058589431468, (8, 8): 18.15310792716475, (1, 7): 15.568152460906626, (0, 9): 7.646274417336397, (3, 4): 58.805202984216365, (2, 4): 16.3730627200805, (8, 4): 29.13716588731284}, 82)\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Question 9\n",
    "\n",
    "Now let's make the dynamics a little more sane again, but let's make the\n",
    "hazard really hazardous. Please create a LargeRescueMDP, set a hazard at `{(1,\n",
    "4)}', set correct_transition_probability to be .76 and set the hazard_cost to\n",
    "be -500 (remember the default values is -100). Then please solve for the\n",
    "optimal value function.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 9:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 9.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to the previous question, with the less noisy dynamics but the\n",
    "greater hazard, the policy didn't change. Why not?\n",
    "\n",
    "<!-- Even thought the dynamics are more deterministic, the expected cost\n",
    "calculation now reflects a higher risk, so it's still cheaper for all the\n",
    "states to the left of (0, 4) to go down and around, to avoid the large\n",
    "negative hazard. -->\n",
    "\n",
    "* At the state (0, 3), you should see that the policy from state (0, 3) still\n",
    "goes to the left, even though it's a much shorter path to the goal to go\n",
    "right. To the nearest 25 (e.g., -225, -250, -275, etc.), what is the largest\n",
    "value of hazard_cost before the agent starts heading left\n",
    "from (0, 3)? (We're expecting you to work this out experimentally, not\n",
    "derive the answer mathematically.)\n",
    "\n",
    "<!-- The arrow flips from -330 to -331, so -325 would be the largest value\n",
    "before it flips.\n",
    "-->\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **5** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_9():\n",
    "  \"\"\"Creates a LargeRescueMDP(), sets a hazard at `{(1, 4)}', the hazard_cost\n",
    "  to -500 and returns the optimal value function and number of iterations\n",
    "  required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vi_test(function, V, rc):\n",
    "  import random; random.seed(0)\n",
    "  import numpy.random as npr; npr.seed(0)\n",
    "  Vsub, rc_sub = function()\n",
    "  assert rc == rc_sub\n",
    "  assert check_value_function(Vsub, V)\n",
    "\n",
    "vi_test(MDP_9, {(5, 9): 37.02794589252677, (4, 7): 36.98725545313149, (6, 9): 42.35713042286265, (7, 3): 53.433329596354355, (4, 8): 36.54316424887878, (3, 0): 14.075905056849074, (0, 2): 6.700885502540759, (2, 8): 42.41475825484247, (5, 6): 80.36920056508337, (6, 6): 69.15258439722244, (7, 7): 51.396035538066826, (0, 7): 37.518097320142125, (2, 1): 10.206624802768253, (8, 0): 29.382372458996578, (8, 9): 34.80542186656143, (1, 6): 47.36876542941666, (3, 7): 42.44149662667289, (0, 3): 3.3135100863236167, (2, 5): 70.2247549867626, (8, 5): 57.12253504752628, (5, 8): 42.191882906121954, (4, 0): 16.35686818974947, (1, 2): 7.930123402395164, (4, 9): 32.664459324932984, (5, 5): 93.29978013077212, (2, 9): 36.64262772070792, (6, 7): 58.798245134724695, (8, 1): 34.07156085430586, (7, 6): 59.520873211555035, (4, 4): 94.26055785356124, (6, 3): 61.24517586208522, (1, 5): 11.24725105614847, (5, 0): 19.22680021712451, (2, 2): 9.123045781763151, (0, 4): -24.263791900481856, (8, 6): 51.81298688248893, (4, 1): 14.091363483396417, (1, 1): 8.840944515572048, (6, 4): 70.00758134919327, (3, 2): 10.468351355958395, (0, 0): 8.783868750560826, (2, 6): 59.29835811332819, (5, 4): 81.27948607782103, (8, 2): 40.07502455981569, (7, 1): 29.049457469952984, (4, 5): 0.0, (6, 0): 22.33983468522747, (1, 4): -31.236013334457546, (3, 9): 32.699939528225556, (0, 5): 28.49945892127023, (7, 5): 66.52022050751522, (8, 7): 45.2410005192894, (4, 2): 12.140432741110777, (1, 0): 10.180308487257106, (0, 8): 32.35363748064205, (6, 5): 78.62603741589356, (3, 5): 93.40286318456691, (0, 1): 7.7177604754815405, (2, 7): 49.78601467266426, (5, 3): 70.09909010172203, (7, 8): 44.34628590495928, (7, 0): 25.643934949764887, (6, 8): 49.49846119918114, (7, 9): 38.673020795121374, (8, 3): 46.565889903167744, (6, 1): 25.091468257369147, (3, 8): 37.069348905756826, (0, 6): 40.036097785064435, (2, 0): 11.956652214396348, (7, 4): 60.30118788620463, (8, 8): 39.486818203417926, (1, 7): 43.28177532777283, (0, 9): 28.22689564401155, (3, 4): 78.0788152179541, (2, 4): 17.47101161965982, (8, 4): 52.49092573982649}, 51)\n",
    "print('Tests passed.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mp04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
