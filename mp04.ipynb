{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "ea100f2b",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "# Miniproject 2"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "7466158c",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aeaa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import abc\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "\n",
    "def press(event):\n",
    "  \"\"\"matplotlib helper function. It processes the keyboard event. If the user hits q, it exit the program.\"\"\"\n",
    "  import matplotlib.pyplot as plt\n",
    "  if event.key == 'q':\n",
    "    sys.exit(0)\n",
    "  plt.close()\n",
    "\n",
    "def plot_value_function(V, mdp):\n",
    "  \"\"\"matplotlib helper function. It takes a value function assumed to be a\n",
    "  maze and plots it.\n",
    "\n",
    "  Args:\n",
    "    V: a dict of {(r, c) : value}\n",
    "  \"\"\"\n",
    "\n",
    "  image = np.zeros((mdp.height, mdp.width))\n",
    "  for state in V.keys():\n",
    "    image[state[0], state[1]] = V[state]\n",
    "\n",
    "  cmap = plt.cm.binary\n",
    "  norm = plt.Normalize(min(V.values()), max(V.values()))\n",
    "  rgba = cmap(norm(image))\n",
    "\n",
    "  for r in range(mdp.height):\n",
    "    for c in range(mdp.width):\n",
    "      if (mdp.obstacles[r][c]):\n",
    "        rgba[r, c, :3] = 1, 0, 0\n",
    "\n",
    "  if mdp.hazards:\n",
    "    for hazard in mdp.hazards:\n",
    "      rgba[hazard[0], hazard[1], :3] = 0, 0, 1\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  fig.canvas.mpl_connect('key_press_event', press)\n",
    "  ax.imshow(rgba)\n",
    "\n",
    "  #\n",
    "  # Note that the image coordinates are in (row, column) order, but the plt.arrow\n",
    "  # command is (x, y). If the rows are y-coordinates, then these two commands\n",
    "  # use different coordinate conventions. Never mind that image coordinates\n",
    "  # also have the origin in the top left corner, and is left-handed.\n",
    "  #\n",
    "  # Computer graphics has much to answer for.\n",
    "  #\n",
    "\n",
    "  for state in V.keys():\n",
    "    r, c = state\n",
    "    delta = mdp.action_to_delta(mdp.get_action(V, state))\n",
    "    plt.arrow(c, r, delta[1]*.35, delta[0]*.35, width = 0.05)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def state_space(self):\n",
    "        \"\"\"Representation of the MDP state set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def action_space(self):\n",
    "        \"\"\"Representation of the MDP action set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        \"\"\"Gamma, defaults to 1.\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        \"\"\"H, defaults to inf.\n",
    "        \"\"\"\n",
    "        return float(\"inf\")\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        \"\"\"Designate certain states as terminal (done) states.\n",
    "\n",
    "        Defaults to False.\n",
    "\n",
    "        Args:\n",
    "            state: A state.\n",
    "\n",
    "        Returns:\n",
    "            is_terminal : A bool.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Return (deterministic) reward for executing action\n",
    "        in state.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "            next_state: A next state.\n",
    "\n",
    "        Returns:\n",
    "            reward : Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        \"\"\"Return a distribution over next states.\n",
    "\n",
    "        The form of this distribution will vary, e.g., depending\n",
    "        on whether the MDP has discrete or continuous states.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "\n",
    "        Returns:\n",
    "            next_state_distribution: Distribution over next states.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def sample_next_state(self, state, action, rng=np.random):\n",
    "        \"\"\"Sample a next state from the transition distribution.\n",
    "\n",
    "        This function may be overwritten by subclasses when the explicit\n",
    "        distribution is too large to enumerate.\n",
    "\n",
    "        Args:\n",
    "            state: A state from the state space.\n",
    "            action: An action from the action space.\n",
    "            rng: A random number generator.\n",
    "\n",
    "        Returns:\n",
    "            next_state: A sampled next state from the state space.\n",
    "        \"\"\"\n",
    "        next_state_dist = self.get_transition_distribution(state, action)\n",
    "        next_states, probs = zip(*next_state_dist.items())\n",
    "        next_state_index = rng.choice(len(next_states), p=probs)\n",
    "        next_state = next_states[next_state_index]\n",
    "        return next_state\n",
    "\n",
    "class RescueMDP(MDP):\n",
    "    \"\"\"A 2D grid Rescue MDP. We know where the person to be rescued is, and we\n",
    "    have to go get them.\"\"\"\n",
    "\n",
    "    _goal_location = (4, 5)\n",
    "    goal_reward = 100\n",
    "    living_reward = 0\n",
    "    hazard_cost = -100\n",
    "    hazards = {}\n",
    "    temporal_discount_factor = .9\n",
    "    goal_is_terminal = True\n",
    "\n",
    "    \"\"\"This is the probability that the action has the intended effect. It\n",
    "    needs to be <= 1. If it is less than 1, the rest of the probability mass\n",
    "    is distributed among the other 3 actions. \"\"\"\n",
    "\n",
    "    _correct_transition_probability = .97\n",
    "    _noise_transition_probability = .01\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "      return np.zeros((2, 3))  # by default, 2x3 grid with no obstacles\n",
    "\n",
    "    @property\n",
    "    def goal_location(self):\n",
    "      return self._goal_location\n",
    "\n",
    "    @goal_location.setter\n",
    "    def goal_location(self, location):\n",
    "      assert location[0] < self.width\n",
    "      assert location[1] < self.height\n",
    "      assert not self.obstacles[location[0]][location[1]]\n",
    "      self._goal_location = location\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "      return self.obstacles.shape[0]\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "      return self.obstacles.shape[1]\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "      return {(r, c) for r in range(self.height) for c in range(self.width)\n",
    "              if not self.obstacles[r][c]}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "      return {'up', 'down', 'left', 'right'}\n",
    "\n",
    "    def action_to_delta(self, action):\n",
    "      return {\n",
    "        'up': (-1, 0),  # up,\n",
    "        'down': (1, 0),  # down,\n",
    "        'left': (0, -1),  # left,\n",
    "        'right': (0, 1),  # right,\n",
    "      }[action]\n",
    "\n",
    "    def get_action(self, V, s):\n",
    "\n",
    "      best_value = -float(\"inf\")\n",
    "      best_action = False\n",
    "\n",
    "      for a in self.action_space:\n",
    "        qsa = 0.\n",
    "        for ns, p in self.get_transition_distribution(s, a).items():\n",
    "          r = self.get_reward(s, a, ns)\n",
    "          qsa += p * (r + self.temporal_discount_factor * V[ns])\n",
    "        if qsa > best_value:\n",
    "          best_value = qsa\n",
    "          best_action = a\n",
    "\n",
    "      return best_action\n",
    "\n",
    "\n",
    "    @property\n",
    "    def correct_transition_probability(self):\n",
    "      return self._correct_transition_probability\n",
    "\n",
    "    @correct_transition_probability.setter\n",
    "    def correct_transition_probability(self, prob):\n",
    "      assert prob >= 0 and prob <= 1\n",
    "\n",
    "      self._correct_transition_probability = prob\n",
    "      self._noise_transition_probability = (1.0 - prob) / 3.0\n",
    "\n",
    "    @property\n",
    "    def noise_transition_probability(self):\n",
    "      return self._noise_transition_probability\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "      # Discrete distributions, represented with a dict\n",
    "      # mapping next states to probs.\n",
    "      next_state_dist = defaultdict(float)\n",
    "\n",
    "      if self.state_is_terminal(state):\n",
    "        return {state: 1.0}\n",
    "\n",
    "      row, col = state\n",
    "      for (dr, dc) in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "        r, c = row + dr, col + dc\n",
    "        # Stay in place if out of bounds or obstacle\n",
    "        if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "          r, c = row, col\n",
    "        elif self.obstacles[r, c]:\n",
    "          r, c = row, col\n",
    "        next_agent_pos = (r, c)\n",
    "        if self.action_to_delta(action) == (dr, dc):\n",
    "          next_state_dist[next_agent_pos] += self.correct_transition_probability\n",
    "        else:\n",
    "          next_state_dist[next_agent_pos] += self.noise_transition_probability\n",
    "\n",
    "      return next_state_dist\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "      agent_pos = next_state\n",
    "      if agent_pos == self.goal_location:\n",
    "        return self.goal_reward\n",
    "      if self.hazards and next_state in self.hazards:\n",
    "        return self.hazard_cost\n",
    "      return self.living_reward\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "      if not self.goal_is_terminal:\n",
    "        return False\n",
    "      return state == self.goal_location\n",
    "\n",
    "def bellman_backup(s, V, mdp):\n",
    "  \"\"\"Look ahead one step and propose an update for the value of s.\n",
    "\n",
    "  You can assume that the mdp is either infinite or indefinite\n",
    "  horizon (that is, mdp.horizon is inf).\n",
    "\n",
    "  Args:\n",
    "    s: A state.\n",
    "    V: A dict, V[state] -> value.\n",
    "    mdp: An MDP.\n",
    "\n",
    "    Returns:\n",
    "      vs: new value estimate for s.\n",
    "  \"\"\"\n",
    "\n",
    "  assert mdp.horizon == float(\"inf\")\n",
    "  vs = -float(\"inf\")\n",
    "  for a in mdp.action_space:\n",
    "    qsa = 0.\n",
    "    for ns, p in mdp.get_transition_distribution(s, a).items():\n",
    "      r = mdp.get_reward(s, a, ns)\n",
    "      qsa += p * (r + mdp.temporal_discount_factor * V[ns])\n",
    "      vs = max(qsa, vs)\n",
    "  return vs\n",
    "\n",
    "def value_iteration(mdp, max_num_iters=1000, change_threshold=1e-4):\n",
    "  \"\"\"Run value iteration for a certain number of iterations or until\n",
    "  the max change between iterations is below a threshold.\n",
    "\n",
    "  You can assume that the mdp is either infinite or indefinite\n",
    "  horizon (that is, mdp.horizon is inf).\n",
    "\n",
    "  Args:\n",
    "    mdp: An MDP.\n",
    "    max_num_iters: An int representing the maximum number of\n",
    "    iterations to run value iteration before giving up.\n",
    "    change_threshold: A float used to determine when value iteration\n",
    "    has converged and it is safe to terminate.\n",
    "\n",
    "  Returns:\n",
    "    V:  A dict, V[state] -> value.\n",
    "    it: The number of iterations before convergence.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize V to all zeros\n",
    "  V = {s: 0. for s in mdp.state_space}\n",
    "\n",
    "  for it in range(max_num_iters):\n",
    "    next_V = {}\n",
    "    max_change = 0.\n",
    "    for s in mdp.state_space:\n",
    "      if mdp.state_is_terminal(s):\n",
    "        next_V[s] = 0.\n",
    "      else:\n",
    "        next_V[s] = bellman_backup(s, V, mdp)\n",
    "      max_change = max(abs(next_V[s] - V[s]), max_change)\n",
    "\n",
    "    V = next_V\n",
    "\n",
    "    if max_change < change_threshold:\n",
    "      break\n",
    "\n",
    "  return V, it\n",
    "\n",
    "\n",
    "class SmallRescueMDP(RescueMDP):\n",
    "  \"\"\"A small 2D grid MDP.\"\"\"\n",
    "\n",
    "  goal_location = (0, 0)\n",
    "  @property\n",
    "  def obstacles(self):\n",
    "    return np.array([\n",
    "      [0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0],\n",
    "      [0, 0, 0, 1, 0],\n",
    "      [0, 1, 0, 1, 1]\n",
    "      ])\n",
    "\n",
    "class LargeRescueMDP(RescueMDP):\n",
    "  \"\"\"A larger 2D grid MDP.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def obstacles(self):\n",
    "    return np.array([\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "      [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "      [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb6d9e",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "a2eafc6d",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "### Warmup 1\n",
    "\n",
    "We have provided a new RescueMDP class for you, that models the robot's motion\n",
    "as noisy. This class is in many ways similar to the ChaseMDP class in homework\n",
    "7, in that it is a maze MDP with obstacles. However, there's just the one\n",
    "agent (the robot, no bunny).\n",
    "\n",
    "Recall that an MDP is defined by the following tuple:\n",
    "* States: The state space is a set of coordinates. The set is defined over a\n",
    "grid, but states that aren't obstacles aren't in the state space.\n",
    "* Actions: The robot can take four actions, up, down, left, right.\n",
    "* A transition function: The probability the action succeeds at moving the\n",
    "robot in the given direction is given by the class field\n",
    "`correct_transition_probability`. If this probability is less than 1, then the\n",
    "rest of the probability mass is uniformly distributed among the other three\n",
    "directions. Any probability mass for a motion that would move the robot into\n",
    "an obstacle or out of the map is mapped to the robot staying the same\n",
    "place. If the robot is in the goal state and the MDP has the flag\n",
    "`goal_is_terminal = True`, it cannot transition out of the goal state. If the flag\n",
    "`goal_is_terminal = False`, the robot is free to leave the goal state and\n",
    "re-enter it.\n",
    "* Reward function: The robot gets a reward of `living_reward` for each action\n",
    "it takes. It gets reward of `goal_reward` every time it enters the goal\n",
    "state.\n",
    "\n",
    "Let's begin solving a small version of the RescueMDP. The maze is 4x5 (remember\n",
    "that arrays are row-major order). The person to be rescued (goal_location) is\n",
    "at (0, 0).\n",
    "\n",
    "In the first warmup problem, please solve\n",
    "for the optimal value function for this problem. We have provided the reference\n",
    "implementation for value iteration. \n",
    "\n",
    "For reference, our solution is **4** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "acd6bac4",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_1():\n",
    "  \"\"\"Creates a SmallRescueMDP() and returns the optimal value function.\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "0c2f1282",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "de89955b",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "1c788def",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "### Warmup 2\n",
    "\n",
    "We also want you to be able to examine the policy that results from solving\n",
    "for the optimal value function. Please use the helper function\n",
    "`plot_value_function` to generate a plot of both the value function and the\n",
    "corresponding policy.\n",
    "\n",
    "<!-- first_experiment() -->\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 1:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 1.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "1db5ccbe",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "### Warmup 3\n",
    "Please hand-code a list of actions that will get the robot from (3,2) to the goal location (0, 0) under the optimal policy. This should be a python list of actions, e.g., [down, down, down, down].\n",
    "\n",
    "For reference, our solution is **2** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "a64e25fb",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup3():\n",
    "  \"\"\"Hand-code a list of actions that the robot will take under the optimal\n",
    "  policy from a start state of (3, 2) to the goal state of (0, 0).\n",
    "\n",
    "  Returns:\n",
    "    actions: A list of str actions that get the robot to the goal state.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "6eb25d64",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "64405772",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "cccde8cf",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "### MDP Question 2\n",
    "\n",
    "In the MDP class we have provided, the goal state is a terminal state. Once\n",
    "the agent is in the terminal state, it can't leave and it doesn't receive any\n",
    "more reward. Let's investigate what happens if we don't make the goal state a\n",
    "terminal state.\n",
    "\n",
    "Please create a SmallRescueMDP, set the goal_is_terminal to be False, and\n",
    "compute the value function.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 2:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 2.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to the previous question, you should have seen the number of\n",
    "iterations change before value iteration converged. Why is that? What extra\n",
    "work are we making value iteration do in this case?\n",
    "\n",
    "<!-- Because the goal state is not terminal, the agent has the option to leave\n",
    "the goal state and return, and accumulate the goal reward over and over\n",
    "again. The future rewards are discounted, and so this accumulation of reward\n",
    "will eventually converge, but the value iteration process needs to run to\n",
    "convergence to add up that geometric series. -->\n",
    "\n",
    "* At the same time, the optimal policy did not change anywhere except possibly\n",
    "the goal. Why is the policy the same, even though it took more iterations to\n",
    "converge?\n",
    "\n",
    "<-- The extra work is really adding up the extra rewards at the goal state, so\n",
    "the policy shouldn't change anywhere else. -->\n",
    "\n",
    "* Is there some property of the reward function in general for MDPs that makes\n",
    "solving for the optimal policy with value iteration easier or harder?\n",
    "\n",
    "<-- If the distribution of states that the policy visits in the limit of time\n",
    "has zero expected reward, then there's no geometric series of discounted\n",
    "rewards to add up. A terminal state with zero reward makes that happen. -->\n",
    "\n",
    "* Optional: In this and the following questions, when we ask you to compare\n",
    "policies, we want you to ignore the goals state.  The value at the goal\n",
    "state will usually not change, but the action returned by the policy might have. Why\n",
    "is that?\n",
    "\n",
    "<-- The goal state, all actions are equal, so the policy returns the first\n",
    "action in the list of mdp.action_space. This is a set, and the order of items\n",
    "returned by a set is not guaranteed in python, so the first item returned is\n",
    "non-deterministic. -->\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **5** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "a5d13348",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_2():\n",
    "  \"\"\"Creates a SmallRescueMDP(), sets goal_is_terminal to False and returns the optimal value function.\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "f4d5831e",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "d95a1c4d",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "7fa322ff",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "### MDP Question 3\n",
    "\n",
    "Let's now turn our attention to a larger MDP. Please create a LargeRescueMDP\n",
    "and compute the optimal policy.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 3:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 3.\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **3** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "06639687",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_3():\n",
    "  \"\"\"Creates a LargeRescueMDP(), and returns the optimal value function and\n",
    "  number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "0e1650f6",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "a2ee1838",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "af982d20",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "### MDP Question 4\n",
    "\n",
    "The default transition model for the LargeRescueMDP is pretty close\n",
    "deterministic. There is a .97 chance that each action succeeds in the intended\n",
    "direction (unless there's an obstacle in the way or its the edge of the map)\n",
    "and only a .01 chance of ending up in one of the other 3 neighbouring grid\n",
    "cells.\n",
    "\n",
    "What happens if we make the transition model noisier? Please create a\n",
    "LargeRescueMDP and set the correct_transition_probability to be .76. This is\n",
    "not too noisy, but it has implications for our ability to solve for the\n",
    "optimal policy.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 4:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 4.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to the previous question, you should have seen the number of\n",
    "iterations change before value iteration converged. Why is that? What extra\n",
    "work are we making value iteration do in this case?\n",
    "\n",
    "<!-- Because the motion to the next state is noisy, it can take more steps to get\n",
    "to the goal. Value iteration converges when the expected reward of the policy\n",
    "from each state to the goal has been computed. Since it takes more steps to\n",
    "get to the goal, it takes more iterations to compute the expected reward. -->\n",
    "\n",
    "* At the same time, the optimal policy did not change. Why didn't the policy\n",
    "change under the noisy dynamics?\n",
    "\n",
    "<!-- There isn't a way for the agent to compensate for the noisy dynamics, so\n",
    "there's no change to the policy that could improve the progress towards the\n",
    "goal. -->\n",
    "\n",
    "* You should have seen the value function decrease in nearly all cells. Why\n",
    "did it go down? Where is the loss in value coming from?\n",
    "\n",
    "<!-- The loss in value at each state comes from the fact that more actions are\n",
    "likely to be required to get to the goal, and so the value of the goal state\n",
    "is likely to be discounted more, and the discounted expected value\n",
    "of a sequence of actions will be reduced. -->\n",
    "\n",
    "* What would happen to the number of iterations, to the policy, and to the\n",
    "value function if the dynamics were even noisier, say\n",
    "correct_transition_probability was set to 0.5?\n",
    "\n",
    "<!-- The number of iterations would go up, the value function would be reduced\n",
    "further, but the policy would not change. -->\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **4** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "b234ff95",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_4():\n",
    "  \"\"\"Creates a LargeRescueMDP(), sets the correct_transition_probability to\n",
    "  .76 and returns the optimal value function and\n",
    "  number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "88357dd9",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "b3646ef3",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "0bb77b52",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "### MDP Question 5\n",
    "\n",
    "The default temporal_discount_factor is .9. This is in general quite a low\n",
    "discount factor. An action 20 steps away will have $.9^{20} \\approx .12$ impact on later\n",
    "actions.\n",
    "\n",
    "What happens if we increase the discount factor? Please create a\n",
    "LargeRescueMDP, set the temporal_discount_factor to be .99, and also set\n",
    "the correct_transition_probability to be .76. Then please solve for the\n",
    "optimal value function.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 5:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 5.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to the previous question, you should have seen the number of\n",
    "iterations increase before value iteration converged. Why is that? What\n",
    "extra work are we making value iteration do in this case?\n",
    "\n",
    "<!-- This is a subtle question. If the value of a state is the expected return\n",
    "of a discounted sequence of rewards, the value function converges when we've\n",
    "done enough backups that the effect of one more backup and one more discounted\n",
    "reward is no longer measurable numerically. When we make the value function\n",
    "close to one, then it takes a longer sequence of backups for the discounted\n",
    "reward to no longer be measurable numerically. -->\n",
    "\n",
    "* At the same time, the optimal policy did not change. Why is the policy the\n",
    "same, but it took more iterations to converge?\n",
    "\n",
    "<!-- There's no reason to do anything different, we're just making the value\n",
    "function compute the expected value of a longer sequence of possible outcomes\n",
    "under the policy. -->\n",
    "\n",
    "* You should have seen the value function increase in nearly all cells. Why\n",
    "did it go up? Where did the increase in value come from?\n",
    "\n",
    "<!-- The increase in value came from the fact that we're not down-weighting\n",
    "the future as much. For the same sequence of actions, we expect to get a\n",
    "higher reward because we value the future more. -->\n",
    "\n",
    "* What would happen to the number of iterations, to the policy, and to the\n",
    "value function if we left the temporal_discount_factor at .99 but made the\n",
    "dynamics fairly deterministic again, say correct_transition_probability = .99?\n",
    "(You should try this!)\n",
    "\n",
    "<!-- The number of iterations will go down because the path is nearly\n",
    "deterministic now --- from each state, the policy will get to the goal in\n",
    "(mostly) a fixed number of steps. The fact that we can take expectations over\n",
    "longer sequences of actions doesn't change the value function or make us\n",
    "compute more iterations because once we get to the goal, we get no more\n",
    "reward. The policy won't change, but the value function will stay high. -->\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **5** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "65066ee6",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_5():\n",
    "  \"\"\"Creates a LargeRescueMDP(), sets the correct_transition_probability to\n",
    "  .76 and the temporal_discount_factor to .99 and returns the optimal value function and\n",
    "  number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "15c92627",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "ccf87db5",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "84f5449d",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "### MDP Question 6\n",
    "\n",
    "Let's make the problem a little harder. In addition to obstacles, our\n",
    "RescueMDP class supports hazards, which are states that have a penalty for\n",
    "entering.\n",
    "\n",
    "Let's start with relatively noise-free dynamics. Please create a\n",
    "LargeRescueMDP and set a hazard at `{(1, 4)}'. Then please solve for the\n",
    "optimal value function.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 6:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 6.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to question three (LargeRescueMDP, discount factor = .9,\n",
    "correct_transition_probability = .97, no hazards), the number of iterations to\n",
    "solve for the optimal value function should be more or less the same, and the\n",
    "policy is the same too. Why didn't the hazard change the policy or require\n",
    "more iterations?\n",
    "\n",
    "<!-- The location of the hazard wasn't on the most likely path to the goal for\n",
    "most states, and with nearly-deterministic dynamics, there was rarely a need\n",
    "to react to the hazard. -->\n",
    "\n",
    "* You should have seen the value function decrease a little bit some but not\n",
    "all states. Where did the value function change and why?\n",
    "\n",
    "<!-- The states where the expected trajectory took the agent past the hazard\n",
    "saw a small reduction in value, due to the small chance that the stochastic\n",
    "dynamcs would take the agent into the hazard, e.g. (0, 0). The states where\n",
    "the expected trajectory took the agent nowhere near the hazard saw no change\n",
    "in value because for those states, the problem looks exactly the same as\n",
    "before. -->\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **4** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "6cf99144",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_6():\n",
    "  \"\"\"Creates a LargeRescueMDP(), and sets a hazard at `{(1, 4)}' and returns\n",
    "  the optimal value function and number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "16f38a5b",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "25c689bb",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "e37bf75b",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "### MDP Question 7\n",
    "\n",
    "Given the hazards, let's make the dynamics noisier. Please create a\n",
    "LargeRescueMDP, set a hazard at `{(1, 4)}' and set\n",
    "correct_transition_probability to be .76. Then please solve for the optimal\n",
    "value function.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 7:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 7.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to the previous question, with these noisier dynamics, the policy\n",
    "changed slightly. Please enumerate which states had a different policy\n",
    "relative to the previous question. Why did the policy change?\n",
    "\n",
    "<!-- The policy changed in states (4,1) and (3,0). For those two states, the\n",
    "risk of encountering the hazard, and the associated penalty, increased enough\n",
    "relative to the last problem that it made more sense to go down and around the\n",
    "red obstacle, rather than up and over. Those two states were right on the\n",
    "borderline of whether the higher value policy was to go up and over or down\n",
    "and under. When the dynamics are deterministic,\n",
    "it's cheaper to go up and over. The increased noise made it cheaper to go down\n",
    "and under and avoid the hazard.\n",
    "\n",
    "-->\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **5** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "98e055cc",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_7():\n",
    "  \"\"\"Creates a LargeRescueMDP(), and sets a hazard at `{(1, 4)}' and returns\n",
    "  the optimal value function and number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "77a719c4",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "c978113e",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "a83cc70b",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "### MDP Question 8\n",
    "\n",
    "Now let's make the hazard really hazardous. Please create a LargeRescueMDP,\n",
    "set a hazard at `{(1, 4)}', set correct_transition_probability to be .76 and\n",
    "set the hazard_cost to be -1000. Then please solve for the optimal value\n",
    "function.\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 8:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 8.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to the previous question, with these noisier dynamics, the policy\n",
    "changed a lot. Where did the policy change (you don't need to enumerate\n",
    "states, just give a general description) and why?\n",
    "\n",
    "<!-- The policy changed in the top left. It's cheaper now for all the states\n",
    "to the left of (0, 4) to go down and around, to avoid the large negative\n",
    "hazard.\n",
    "-->\n",
    "\n",
    "* Compared to the previous question, value iteration should have taken a\n",
    "little longer. What extra work are we making value iteration do in this\n",
    "case?\n",
    "\n",
    "<!-- Because the policy is taking the states from the top left of the state\n",
    "space down and around the obstacles, more iterations are required to compute\n",
    "the expected rewards for those states. -->\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **5** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "e0b81870",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_8():\n",
    "  \"\"\"Creates a LargeRescueMDP(), sets the correct_transition_probability to\n",
    "  .76 and the temporal_discount_factor to .99 and returns the optimal value function and\n",
    "  number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "78fd1a93",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "c2e45077",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "acae2711",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "### MDP Question 9\n",
    "\n",
    "What if we make the hazard even more hazardous, but have nearly-deterministic dynamics?\n",
    "Please create a LargeRescueMDP, set a hazard at `{(1, 4)}', leave the\n",
    "correct_transition_probability at its default value, but set the hazard_cost to be\n",
    "-10000. Then please solve for the optimal value function.\n",
    "\n",
    "\n",
    "<div class=\"question question-multiplechoice\">\n",
    "<b>Submission Material 9:</b> In your submitted pdf, please include a single\n",
    "plot of the value function with the action under the optimal policy drawn for\n",
    "each state. Name this figure as Figure 9.\n",
    "\n",
    "In your pdf, please also answer the following questions:\n",
    "\n",
    "* Compared to the previous question, with these cleaner dynamics, the policy\n",
    "didn't change. Why is that?\n",
    "\n",
    "<!-- Even with nearly deterministic dynamics, the cost of the hazard is so\n",
    "high that the agent would still prefer to avoid the hazard at all costs.\n",
    "-->\n",
    "\n",
    "* Compared to the previous question, value iteration should have been\n",
    "faster. Why is that?\n",
    "\n",
    "<!-- Because the dynamics are nearly-deterministic, the path to the goal from\n",
    "each state is going to be shorter. Value iteration needs to account for fewer\n",
    "contingencies. -->\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "For reference, our solution is **5** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "e7958cae",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDP_9():\n",
    "  \"\"\"Creates a LargeRescueMDP(), sets the correct_transition_probability to\n",
    "  .76 and the temporal_discount_factor to .99 and returns the optimal value function and\n",
    "  number of iterations required to converge\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    value function: a dict of states to values\n",
    "    it: the number of iterations required to solve for the value function\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "9f83712a",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "980756c1",
=======
>>>>>>> df95f22ca869bb56f5bb59396c8e3884c32d3969
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tests passed.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mp04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
