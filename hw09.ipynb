{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87412b49",
   "metadata": {},
   "source": [
    "# Homework 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c2325",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35493fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import sqrt, log\n",
    "import abc\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def state_space(self):\n",
    "        \"\"\"Representation of the MDP state set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def action_space(self):\n",
    "        \"\"\"Representation of the MDP action set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        \"\"\"Gamma, defaults to 1.\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        \"\"\"H, defaults to inf.\n",
    "        \"\"\"\n",
    "        return float(\"inf\")\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        \"\"\"Designate certain states as terminal (done) states.\n",
    "\n",
    "        Defaults to False.\n",
    "\n",
    "        Args:\n",
    "            state: A state.\n",
    "\n",
    "        Returns:\n",
    "            state_is_terminal : A bool.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_reward(self, state, action, next_state=None):\n",
    "        \"\"\"Return (deterministic) reward for executing action\n",
    "        in state.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "            next_state: Optional. A next state.\n",
    "\n",
    "        Returns:\n",
    "            reward : Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        \"\"\"Return a distribution over next states.\n",
    "\n",
    "        The form of this distribution will vary, e.g., depending\n",
    "        on whether the MDP has discrete or continuous states.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "\n",
    "        Returns:\n",
    "            next_state_distribution: Distribution over next states.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def sample_next_state(self, state, action, rng=np.random):\n",
    "        \"\"\"Sample a next state from the transition distribution.\n",
    "\n",
    "        This function may be overwritten by subclasses when the explicit\n",
    "        distribution is too large to enumerate.\n",
    "\n",
    "        Args:\n",
    "            state: A state from the state space.\n",
    "            action: An action from the action space.\n",
    "            rng: A random number generator.\n",
    "\n",
    "        Returns:\n",
    "            next_state: A sampled next state from the state space.\n",
    "        \"\"\"\n",
    "        next_state_dist = self.get_transition_distribution(state, action)\n",
    "        next_states, probs = zip(*next_state_dist.items())\n",
    "        next_state_index = rng.choice(len(next_states), p=probs)\n",
    "        next_state = next_states[next_state_index]\n",
    "        return next_state\n",
    "\n",
    "\n",
    "class POMDP(MDP):\n",
    "    \"\"\"A partially observable Markov decision process (POMDP).\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def observation_space(self):\n",
    "        \"\"\"Representation of the POMDP observation space.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractclassmethod\n",
    "    def get_observation_distribution(self, next_state, action):\n",
    "        \"\"\"Return a distribution over the observations.\n",
    "\n",
    "        The form of this distribution will vary, e.g., depending\n",
    "        on whether the MDP has discrete or continuous observation\n",
    "        spaces.\n",
    "\n",
    "        Args:\n",
    "            next_state: The next state.\n",
    "            action: The action taken.\n",
    "\n",
    "        Returns:\n",
    "            observation_distribution: Distribution over the observation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "\n",
    "class LambdaMDP(MDP):\n",
    "    \"\"\"A helper class that creates a MDP class based on a set of functions.\n",
    "    See the constructor for details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_space, action_space, state_is_terminal_fn, get_reward_fn, get_transition_distribution_fn, temporal_discount_factor=1.0):\n",
    "        \"\"\"\n",
    "        Construct a MDP class based on a set of function definitions.\n",
    "\n",
    "        Args:\n",
    "            state_space: The set of possible states.\n",
    "            action_space: The set of possible actions.\n",
    "            state_is_terminal_fn: A callable function: state_is_terminal_fn(state) -> bool,\n",
    "                mapping a state to a boolean value indicating whether\n",
    "                the state is a terminal state.\n",
    "            get_reward_fn: A callable function: get_reward_fn(state, action, next_state) -> float,\n",
    "                mapping a (s, a, s') tuple to a float reward value.\n",
    "            get_transition_distribution_fn: A callable function:\n",
    "                get_transition_distribution_fn(state, action) -> distribution of the next state.\n",
    "                Note that the return value for this function must be a discrete distribution.\n",
    "            temporal_discount_factor: A float number, the temporal discount factor of the MDP.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.state_space_v = state_space\n",
    "        self.action_space_v = action_space\n",
    "        self.state_is_terminal = state_is_terminal_fn\n",
    "        self.get_reward = get_reward_fn\n",
    "        self.get_transition_distribution = get_transition_distribution_fn\n",
    "        self.temporal_discount_factor_v = temporal_discount_factor\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return self.state_space_v\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self.action_space_v\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return self.temporal_discount_factor_v\n",
    "\n",
    "\n",
    "class DiscreteDistribution(object):\n",
    "    \"\"\"A discrete distribution, represneted as a dictionary.\"\"\"\n",
    "\n",
    "    eps = 1e-6\n",
    "\n",
    "    def __init__(self, prob_dict, support=None):\n",
    "        \"\"\"Construct a discrete distribution based on the support set\n",
    "        and the probability dictionary. The dictionary might be sparse,\n",
    "        in which case the omitted entries are treated as zero-probability\n",
    "        values.\n",
    "\n",
    "        The support argument denotes the set of possible values for the\n",
    "        random variable. It can be left as optional, especially when the underlying\n",
    "        support set is a continuous set (e.g., all real numbers). Note\n",
    "        that, even in this case, we can still define a \"discrete distribution\",\n",
    "        that is, a distribution only has mass on a finite set of points.\n",
    "        For example, we can define a distribution on R: {0: 0.5, 1: 0.5}.\n",
    "        Implicitly, all values not in the prob_dict will be treated as\n",
    "        zero-probability.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```\n",
    "        p = DiscreteDistribution({'x': 0.3, 'y': 0.3, 'z': 0.4}, {'x', 'y', 'z'})\n",
    "        print(p.p('x'))  # 0.3\n",
    "        for x in p:  # iterate over the set of possible values.\n",
    "            print(p.p(x))  # should print x, 0.3, y, 0.3, z, 0.4\n",
    "        for x, p_x in p.items():  # just like iterating over a Python dict.\n",
    "            print(x, p_x)  # should print x, 0.3, y, 0.3, z, 0.4\n",
    "        ```\n",
    "\n",
    "        Args:\n",
    "            prob_dict: A dictionary, mapping elements in support to a float\n",
    "                number. The dictionary might be sparse. It should always\n",
    "                sum up to one (thus being a valid distribution.)\n",
    "            support (Optional): A set of objects.\n",
    "        \"\"\"\n",
    "        assert type(prob_dict) is dict and type(support) in (type(None), set)\n",
    "\n",
    "        self.prob_dict = prob_dict\n",
    "        self.support = support\n",
    "\n",
    "        if self.support is not None:\n",
    "            for k in self.prob_dict:\n",
    "                assert k in self.support\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate over the support set.\"\"\"\n",
    "        yield from self.prob_dict\n",
    "\n",
    "    def items(self):\n",
    "        \"\"\"Iterate over the distribution. Generates a list of (x, p(x)) pairs.\n",
    "        This function will ignore zero-probability values in the support.\n",
    "        \"\"\"\n",
    "        yield from self.prob_dict.items()\n",
    "\n",
    "    def p(self, value):\n",
    "        \"\"\"Evaluate the proabbility of a value in the support set.\n",
    "\n",
    "        Args:\n",
    "            value: An object in the support set.\n",
    "\n",
    "        Returns:\n",
    "            p: A float, indicating p(value).\n",
    "        \"\"\"\n",
    "        if self.support is not None:\n",
    "            assert value in self.support\n",
    "        return self.prob_dict.get(value, 0.)\n",
    "\n",
    "    def renormalize(self):\n",
    "        \"\"\"Renormalize the distribution to ensure that the probabilities sum up to 1.\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        z = sum(self.prob_dict.values())\n",
    "        assert z > 0, 'Degenerated probability distribution.'\n",
    "        self.prob_dict = {k: v / z for k, v in self.prob_dict.items()}\n",
    "        return self\n",
    "\n",
    "    def check_normalization(self):\n",
    "        \"\"\"Check if the prob dict is correctly normalized (i.e., should sum up to 1).\"\"\"\n",
    "        assert 1 - type(self).eps < sum(self.prob_dict.values()) < 1 + type(self).eps\n",
    "\n",
    "    def max(self):\n",
    "        \"\"\"Return max_x p(x) and argmax_x p(x).\n",
    "\n",
    "        Returns:\n",
    "            p_max: A float, max_x p(x).\n",
    "            arg_max: An object in the support, argmax_x p(x).\n",
    "        \"\"\"\n",
    "        return max((v, k) for k, v in self.prob_dict.items())\n",
    "\n",
    "    def draw(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = np.random\n",
    "        keys = list(self.prob_dict.keys())\n",
    "        probs = [self.prob_dict[k] for k in keys]\n",
    "        return keys[rng.choice(len(keys), p=probs)]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.prob_dict)\n",
    "\n",
    "\n",
    "def OnehotDiscreteDistribution(support, obj):\n",
    "    \"\"\"Create a DiscreteDistribution of support. p(obj) = 1.\"\"\"\n",
    "    return DiscreteDistribution({obj: 1.0}, support=support)\n",
    "\n",
    "\n",
    "def UniformDiscreteDistribution(support):\n",
    "    \"\"\"Create a DiscreteDistribution that is uniform. That is, for any object x, p(x) = 1 / |support|.\"\"\"\n",
    "    return DiscreteDistribution({x: 1 / len(support) for x in support}, support=support)\n",
    "\n",
    "# Our RobotChargingPOMDP\n",
    "\n",
    "class RobotChargingPOMDP(POMDP):\n",
    "    DEF_MOVE_SUCCESS = 0.8\n",
    "    DEF_OBS_IF_THERE = 0.9\n",
    "    DEF_OBS_IF_NOT_THERE = 0.4\n",
    "    DEF_C_MOVE = 0.5\n",
    "    DEF_C_LOOK = 0.1\n",
    "    DEF_GAMMA = 0.9\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        p_move_success=DEF_MOVE_SUCCESS, p_obs_if_there=DEF_OBS_IF_THERE, p_obs_if_not_there=DEF_OBS_IF_NOT_THERE,\n",
    "        c_move=DEF_C_MOVE, c_look=DEF_C_LOOK,\n",
    "        gamma=DEF_GAMMA\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create the Robot Chargin POMDP.\n",
    "\n",
    "        Args:\n",
    "            p_move_success (float): the probability that a move action is successful.\n",
    "            p_obs_if_there (float): the probability of return 1 when looking at a location with the charger.\n",
    "            p_obs_if_not_there (float): the probability of return 1 when looking at a location without a charger.\n",
    "            c_move (float): the cost of a move action.\n",
    "            c_look (float): the cost of a look action.\n",
    "            gamma (float): the temporal discount factor.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.p_move_success = p_move_success\n",
    "        self.p_obs_if_there = p_obs_if_there\n",
    "        self.p_obs_if_not_there = p_obs_if_not_there\n",
    "        self.c_move = c_move\n",
    "        self.c_look = c_look\n",
    "        self.gamma = gamma\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        \"\"\"\n",
    "        Three \"normal\" states: 0, 1, 2, indicating the position of the charger.\n",
    "        One \"terminal\" state T. Executing the \"charge\" action will reach this\n",
    "        terminal state. And the state is absorbing. The robot will deterministically\n",
    "        transition to this terminal state when we execute the c action.\n",
    "        \"\"\"\n",
    "        return {0, 1, 2, 'T'}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        # lx: look(x)\n",
    "        # mxy: move(start=x, target=y)\n",
    "        # c: charge\n",
    "        # nop: NOP\n",
    "        return {'l0', 'l1', 'l2', 'm01', 'm12', 'm20', 'c', 'nop'}\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return {0, 1}\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return self.gamma\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        return state == 'T'\n",
    "\n",
    "    def get_reward(self, state, action, next_state=None):\n",
    "        if action == 'nop':\n",
    "            return 0\n",
    "        elif action == 'c':\n",
    "            if state == 0:\n",
    "                return 10\n",
    "            else:\n",
    "                return -100\n",
    "        elif action.startswith('m'):\n",
    "            return -self.c_move\n",
    "        else:  # look\n",
    "            return -self.c_look\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        if action == 'c':\n",
    "            return OnehotDiscreteDistribution(self.state_space, 'T')\n",
    "        elif action.startswith('m'):\n",
    "            start, target = int(action[1]), int(action[2])\n",
    "            if state == start:\n",
    "                return DiscreteDistribution({target : self.p_move_success, start : 1 - self.p_move_success}, support=self.state_space)\n",
    "        return OnehotDiscreteDistribution(self.state_space, state)\n",
    "\n",
    "    def get_observation_distribution(self, next_state, action):\n",
    "        if action.startswith('l'):\n",
    "            target = int(action[1])\n",
    "            if next_state == target:\n",
    "                return DiscreteDistribution({0: 1 - self.p_obs_if_there, 1: self.p_obs_if_there}, support=self.observation_space)\n",
    "            else:\n",
    "                return DiscreteDistribution({0: 1 - self.p_obs_if_not_there, 1: self.p_obs_if_not_there}, support=self.observation_space)\n",
    "        return OnehotDiscreteDistribution(self.observation_space, 0)\n",
    "\n",
    "\n",
    "def expectimax_search(initial_state, mdp, horizon, most_likely_state=False, return_Q=False):\n",
    "    '''Use expectimax search to determine a next action.\n",
    "\n",
    "    Note that we're just computing the single next action to\n",
    "    take, we do not need to store the entire partial V.\n",
    "\n",
    "    Horizon is given as a separate argument so that we can use\n",
    "    expectimax search with receding horizon control, for example,\n",
    "    even if mdp.horizon is inf.\n",
    "\n",
    "    Args:\n",
    "        initial_state: A state in the mdp.\n",
    "        mdp: An MDP.\n",
    "        horizon: An int horizon.\n",
    "        most_likely_state: A boolean value.\n",
    "            If true, compute Q based on the most likely state.\n",
    "        return_Q: A boolean value. If true, also return the Q value\n",
    "            at the root instead of the action.\n",
    "\n",
    "    Returns:\n",
    "        action: An action in the mdp.\n",
    "        Q: The Q value at the root state (only when return_Q is True).\n",
    "    '''\n",
    "    A = mdp.action_space\n",
    "    R = mdp.get_reward\n",
    "    P = mdp.get_transition_distribution\n",
    "    gm = mdp.temporal_discount_factor\n",
    "    ts = mdp.state_is_terminal\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def V(s, h):\n",
    "        if h == horizon or ts(s):\n",
    "            return 0\n",
    "        return max(Q(s, a, h) for a in A)\n",
    "\n",
    "    def Q(s, a, h):\n",
    "        psa = P(s, a)\n",
    "        if most_likely_state:\n",
    "            ns = max((v, k) for k, v in psa.items())[1]\n",
    "            return R(s, a, ns) + gm * V(ns, h+1)\n",
    "        else:\n",
    "            next_states = psa\n",
    "            return sum(psa.p(ns) * (R(s, a, ns) + gm * V(ns, h+1)) for ns in next_states)\n",
    "\n",
    "    Q_values = {a: Q(initial_state, a, 0) for a in A}\n",
    "    if return_Q:\n",
    "        return max(A, key=Q_values.get), Q_values\n",
    "    return max(A, key=Q_values.get)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec7fda",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79af9f",
   "metadata": {},
   "source": [
    "### Transition Update\n",
    "Complete the implementation of the transition update function over beliefs.\n",
    "\n",
    "For reference, our solution is **8** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e380ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_update(pomdp, prior, action):\n",
    "  \"\"\"Compute p(s') from a prior distribution of p(s) based on the transition\n",
    "  distribution p(s, action, s').\n",
    "\n",
    "  Args:\n",
    "      pomdp (POMDP): A POMDP object.\n",
    "      prior (DiscreteDistribution): A distribution over the current state s.\n",
    "      action: The action to be executed.\n",
    "\n",
    "  Returns:\n",
    "      updated_prior (DiscreteDistribution): A distribution over the next state s'.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11540a1a",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2168b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_transition_update():\n",
    "    pomdp = RobotChargingPOMDP()\n",
    "    b0 = DiscreteDistribution({0: 0.4, 1: 0.3, 2: 0.3}, pomdp.state_space)\n",
    "    b1 = transition_update(pomdp, b0, 'm01')\n",
    "    assert np.allclose(b1.p(0), 0.08)\n",
    "    assert np.allclose(b1.p(1), 0.62)\n",
    "    assert np.allclose(b1.p(2), 0.3)\n",
    "\n",
    "test1_transition_update()\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506181f",
   "metadata": {},
   "source": [
    "### Observation Update\n",
    "Complete the implementation of the observation update function over beliefs.\n",
    "\n",
    "For reference, our solution is **4** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b369eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_update(pomdp, prior, action, observation):\n",
    "  \"\"\"Compute p(s' | observation, action) following the Bayes rule.\n",
    "      p(s' | o, a) is proportional to p(s' | a) * p(o | s', a).\n",
    "\n",
    "  Args:\n",
    "      pomdp (POMDP): A POMDP object.\n",
    "      prior (DiscreteDistribution): The prior distribution over the next state: p(s' | a).\n",
    "          Typically, this is the output of the transition_update() function.\n",
    "      action: The action taken.\n",
    "      observation: The observation.\n",
    "\n",
    "  Returns:\n",
    "      posterior (DiscreteDistribution): The posterior distribution over the next state s'.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55700482",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2385ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_observation_update():\n",
    "    pomdp = RobotChargingPOMDP()\n",
    "    b0 = DiscreteDistribution({0: 0.4, 1: 0.3, 2: 0.3}, pomdp.state_space)\n",
    "    b1 = transition_update(pomdp, b0, 'l0')\n",
    "    b1 = observation_update(pomdp, b1, 'l0', 0)\n",
    "    assert np.allclose(b1.p(0), 0.1)\n",
    "    assert np.allclose(b1.p(1), 0.45)\n",
    "    assert np.allclose(b1.p(2), 0.45)\n",
    "\n",
    "test1_observation_update()\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88145df",
   "metadata": {},
   "source": [
    "### Belief Filter\n",
    "Complete the implementation of the belief filter function.\n",
    "\n",
    "For reference, our solution is **5** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def belief_filter(pomdp, belief, action, observation):\n",
    "  \"\"\"Compute the updated belief over the states based on the current action and obervation.\n",
    "\n",
    "  Specifically, the process is:\n",
    "      1. the agent is at state s, and has a belief about its current state p(s).\n",
    "      2. the agent takes an action a, and has a belief about its next state p(s' | a),\n",
    "          computed by transition_update.\n",
    "      3. the agent observes o, which follows the observation model of the POMDP p(o | s', a).\n",
    "      4. the agent updates its belief over the next state p(s' | o, a), following the Bayes rule.\n",
    "\n",
    "  Args:\n",
    "      pomdp (POMDP): A POMDP object.\n",
    "      belief (DiscreteDistribution): The belief about the agent's current state.\n",
    "      action: The action taken.\n",
    "      observation: The observation.\n",
    "\n",
    "  Returns:\n",
    "      next_belief: The belief about the next state by taking into consideration the action\n",
    "          at this step and the observation.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6448a9f",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_belief_filter():\n",
    "    pomdp = RobotChargingPOMDP()\n",
    "    b0 = DiscreteDistribution({0: 0.9, 1: 0.05, 2: 0.05}, pomdp.state_space)\n",
    "    b1 = belief_filter(pomdp, b0, 'l0', 0)\n",
    "    assert np.allclose(b1.p(0), 0.6)\n",
    "    assert np.allclose(b1.p(1), 0.2)\n",
    "    assert np.allclose(b1.p(2), 0.2)\n",
    "\n",
    "test1_belief_filter()\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1676b91d",
   "metadata": {},
   "source": [
    "### Belief-Space MDP\n",
    "In this seciton, you will implement a function `create_belief_mdp`, that transforms a POMDP into a belief-space MDP.\n",
    "    We have provided the basic skeleton for you. In particular, you only need to implement the get_reawrd and the get_transition_distribution\n",
    "    function for the Belief MDP.\n",
    "\n",
    "For reference, our solution is **79** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d142fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_belief_mdp(pomdp):\n",
    "  \"\"\"Constructs a belief-space MDP from a POMDP.\n",
    "\n",
    "  Args:\n",
    "      pomdp: The input POMDP object.\n",
    "\n",
    "  Returns:\n",
    "      belief_mdp: The constructed belief-space MDP.\n",
    "  \"\"\"\n",
    "    def state_is_terminal(belief):\n",
    "        \"\"\"The state_is_terminal function for the belief-space MDP. It returns true iff. all possible states\n",
    "        in the belief are terminal states.\n",
    "\n",
    "        Args:\n",
    "            belief: A DiscreteDistribution of the state.\n",
    "\n",
    "        Returns:\n",
    "            is_terminal: Whether the current belief is a \"terminal\" belief.\n",
    "        \"\"\"\n",
    "        for state, p in belief.items():\n",
    "            if p > 0 and not pomdp.state_is_terminal(state):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def get_reward(belief, action, next_belief=None):\n",
    "        \"\"\"Compute the expected reward function for the belief-space MDP.\n",
    "\n",
    "        You only need to implement the case where the original reward function only\n",
    "        depends on the state and the action (but not the next state).\n",
    "\n",
    "        In this case, the reward function of the belief-space MDP will be only a function\n",
    "        of belief and action, but not next_belief.\n",
    "\n",
    "        In general (where the reward function if a function of state, action, and next_action),\n",
    "        in order to compute the expected reward, we need to also marginalize over the next state\n",
    "        distribution (which is next_belief).\n",
    "\n",
    "        Args:\n",
    "            belief: A DiscreteDistribution of the state.\n",
    "            action: An action.\n",
    "            next_belief: A DiscreteDistribution of the next state. Should be ignored (see above).\n",
    "\n",
    "        Returns:\n",
    "            reward: the expected reward at this step.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_transition_distribution(belief, action):\n",
    "        \"\"\"Compute the transition distribution for an input belief and an action.\n",
    "\n",
    "        Specifically, the output will be a distribution over beliefs. That is, a distribution over\n",
    "        distributions. Since we have restricted our observation space to be finite, the\n",
    "        possible next belief is also a finite space. Thus, we can still use a DiscreteDistribution\n",
    "        object to represent the distribution over the next belief.\n",
    "\n",
    "        Args:\n",
    "            belief: A DiscreteDistribution of the state.\n",
    "            action: An action.\n",
    "\n",
    "        Returns:\n",
    "            next_belief: A DiscreteDistribution of the next state.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # Construct a new MDP based on the functions defined above.\n",
    "    return LambdaMDP(\n",
    "        state_space=None,  # We are not going to specify the state space explicitly (it's a continuous space).\n",
    "        action_space=pomdp.action_space,\n",
    "        state_is_terminal_fn=state_is_terminal,\n",
    "        get_reward_fn=get_reward,\n",
    "        get_transition_distribution_fn=get_transition_distribution,\n",
    "        temporal_discount_factor=pomdp.temporal_discount_factor\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc30fe",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1abe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_create_belief_mdp():\n",
    "    pomdp = RobotChargingPOMDP()\n",
    "    belief_mdp = create_belief_mdp(pomdp)\n",
    "    b4 = DiscreteDistribution({0 : .4, 1: .3, 2: .3}, support=pomdp.state_space)\n",
    "    a, Q = expectimax_search(b4, belief_mdp, 4, return_Q=True)\n",
    "    print(a, Q)\n",
    "\n",
    "    assert a == 'l1'\n",
    "    gt = {'l0': -0.1, 'l1': 0.255914, 'l2': -0.1, 'm12': -0.471128, 'm01': -0.5, 'm20': -0.031586, 'c': -56.0, 'nop': 0.0}\n",
    "    for k, v in gt.items():\n",
    "        assert k in Q and np.allclose(v, Q[k])\n",
    "\n",
    "test1_create_belief_mdp()\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc77c0b",
   "metadata": {},
   "source": [
    "### Receding Horizon Control\n",
    "Use the RHC implementation provided in the colab and answer the questions in catsoop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853d58b",
   "metadata": {},
   "source": [
    "A simple implmentation of the Receding Horizon Control (RHC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dedf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def receding_horizon_control(pomdp, initial_belief, h=4, n=4, mlo=False):\n",
    "    \"\"\"\n",
    "    Simulate a receding horizon control.\n",
    "\n",
    "    Specifically, the function simulates the RHC for n time steps.\n",
    "    We will keep track of two variables.\n",
    "        - robot_b, which is the current belief.\n",
    "        - real_s: the \"true\" state of the world.\n",
    "\n",
    "    Args:\n",
    "        pomdp: The input POMDP problem.\n",
    "        initial_belief (DiscreteDistribution): a distribution of the state (the initial belief).\n",
    "        h (int): The receding horizon.\n",
    "        n (int): The nunber of simulation steps.\n",
    "        mlo (bool): Use the Most-Likely-Observation approximation during the test. This is equivalent\n",
    "            to using most-likely-state approximation on the Belief MDP! (Think about why)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the Belief MDP.\n",
    "    bmdp = create_belief_mdp(pomdp)\n",
    "\n",
    "    robot_b = initial_belief\n",
    "    real_s = robot_b.draw()\n",
    "\n",
    "    print('Robot belief:', robot_b)\n",
    "    print('Real state:', real_s)\n",
    "    print('')\n",
    "\n",
    "    for t in range(n):\n",
    "        print('Step', t)\n",
    "        # expectimax returns the optimal action at the current belief.\n",
    "        a = expectimax_search(robot_b, bmdp, h, most_likely_state=mlo)\n",
    "\n",
    "        print('  Executing:', a)\n",
    "        real_s = pomdp.get_transition_distribution(real_s, a).draw()\n",
    "        print('  Real State:', real_s)\n",
    "        if pomdp.state_is_terminal(real_s):\n",
    "            print('Terminated.')\n",
    "            break\n",
    "        o = pomdp.get_observation_distribution(real_s, a).draw()\n",
    "        print('  Observation:', o)\n",
    "        robot_b = belief_filter(pomdp, robot_b, a, o)\n",
    "        print('  Robot belief:', robot_b)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw09.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
