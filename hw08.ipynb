{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c24dcd",
   "metadata": {},
   "source": [
    "# Homework 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bc6ac",
   "metadata": {},
   "source": [
    "## Imports and Utilities\n",
    "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import sqrt, log\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def state_space(self):\n",
    "        \"\"\"Representation of the MDP state set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def action_space(self):\n",
    "        \"\"\"Representation of the MDP action set.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        \"\"\"Gamma, defaults to 1.\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        \"\"\"H, defaults to inf.\n",
    "        \"\"\"\n",
    "        return float(\"inf\")\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        \"\"\"Designate certain states as terminal (done) states.\n",
    "\n",
    "        Defaults to False.\n",
    "\n",
    "        Args:\n",
    "            state: A state.\n",
    "\n",
    "        Returns:\n",
    "            state_is_terminal : A bool.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"Return (deterministic) reward for executing action\n",
    "        in state.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "            next_state: A next state.\n",
    "\n",
    "        Returns:\n",
    "            reward : Single time step reward.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        \"\"\"Return a distribution over next states.\n",
    "\n",
    "        The form of this distribution will vary, e.g., depending\n",
    "        on whether the MDP has discrete or continuous states.\n",
    "\n",
    "        Args:\n",
    "            state: A current state.\n",
    "            action: An action.\n",
    "\n",
    "        Returns:\n",
    "            next_state_distribution: Distribution over next states.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def sample_next_state(self, state, action, rng=np.random):\n",
    "        \"\"\"Sample a next state from the transition distribution.\n",
    "\n",
    "        This function may be overwritten by subclasses when the explicit\n",
    "        distribution is too large to enumerate.\n",
    "\n",
    "        Args:\n",
    "            state: A state from the state space.\n",
    "            action: An action from the action space.\n",
    "            rng: A random number generator.\n",
    "\n",
    "        Returns:\n",
    "            next_state: A sampled next state from the state space.\n",
    "        \"\"\"\n",
    "        next_state_dist = self.get_transition_distribution(state, action)\n",
    "        next_states, probs = zip(*next_state_dist.items())\n",
    "        next_state_index = rng.choice(len(next_states), p=probs)\n",
    "        next_state = next_states[next_state_index]\n",
    "        return next_state\n",
    "\n",
    "\n",
    "class SingleRowMDP(MDP):\n",
    "    \"\"\"A 1D grid MDP for debugging. The grid is 1x5\n",
    "    and the agent is meant to start off in the middle.\n",
    "    There is +10 reward on the rightmost square, -10 on\n",
    "    the left. Actions are left and right. An action effect\n",
    "    is reversed with 10% probability.\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return {0, 1, 2, 3, 4}  # position in grid\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {0, 1}  # left, right\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Discrete distributions, represented with a dict\n",
    "        # mapping next states to probs.\n",
    "        delta = 1 if action == 1 else -1\n",
    "        intended_effect = min(max(state + delta, 0), 4)\n",
    "        opposite_effect = min(max(state - delta, 0), 4)\n",
    "        assert (intended_effect != opposite_effect)\n",
    "        return {intended_effect: 0.9, opposite_effect: 0.1}\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if next_state == 0:\n",
    "          return -10\n",
    "        if next_state == 4:\n",
    "          return 10\n",
    "        return -1  # living penalty\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        return state in {0, 4}\n",
    "\n",
    "\n",
    "class MarshmallowMDP(MDP):\n",
    "    \"\"\"The Marshmallow MDP described in lecture.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        # (hunger level, marshmallow remains)\n",
    "        return {(h, m) for h in {0, 1, 2} for m in {True, False}}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {\"eat\", \"wait\"}\n",
    "\n",
    "    @property\n",
    "    def horizon(self):\n",
    "        return 4\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        next_hunger_level = next_state[0]\n",
    "        return -(next_hunger_level**2)\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Update marshmallow deterministically\n",
    "        if action == \"eat\":\n",
    "            next_m = False\n",
    "        else:\n",
    "            next_m = state[1]\n",
    "\n",
    "        # Initialize next state distribution dict\n",
    "        # Any state not included assumed to have 0 prob\n",
    "        dist = defaultdict(float)\n",
    "\n",
    "        # Update hunger\n",
    "        if action == \"wait\" or state[1] == False:\n",
    "            # With 0.75 probability, hunger stays the same\n",
    "            dist[(state[0], next_m)] += 0.75\n",
    "            # With 0.25 probability, hunger increases by 1\n",
    "            dist[(min(state[0] + 1, 2), next_m)] += 0.25\n",
    "\n",
    "        else:\n",
    "            assert action == \"eat\" and state[1] == True\n",
    "            # Hunger deterministically set to 1 after eating\n",
    "            dist[(0, next_m)] = 1.0\n",
    "\n",
    "        return dist\n",
    "\n",
    "\n",
    "class ZitsMDP(MDP):\n",
    "    \"\"\"The Zits MDP described in lecture.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return {0, 1, 2, 3, 4}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {\"apply\", \"sleep\"}\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return 0.9\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if action == \"apply\":\n",
    "            return -1 - next_state\n",
    "        assert action == \"sleep\"\n",
    "        return -next_state\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        if action == \"apply\":\n",
    "            return {\n",
    "                0: 0.8,\n",
    "                4: 0.2\n",
    "            }\n",
    "        assert action == \"sleep\"\n",
    "        return {\n",
    "            min(state + 1, 4): 0.4,\n",
    "            max(state - 1, 0): 0.6\n",
    "        }\n",
    "\n",
    "\n",
    "class ChaseMDP(MDP):\n",
    "    \"\"\"A 2D grid bunny chasing MDP.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "        return np.zeros((2, 3))  # by default, 2x3 grid with no obstacles\n",
    "\n",
    "    @property\n",
    "    def goal_reward(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def living_reward(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "        return self.obstacles.shape[0]\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "        return self.obstacles.shape[1]\n",
    "\n",
    "    @property\n",
    "    def state_space(self):\n",
    "        pos = [(r, c) for r in range(self.height) for c in range(self.width)]\n",
    "        return {(p1, p2) for p1 in pos for p2 in pos}\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return {'up', 'down', 'left', 'right'}\n",
    "\n",
    "    @property\n",
    "    def temporal_discount_factor(self):\n",
    "        return 0.9\n",
    "\n",
    "    def action_to_delta(self, action):\n",
    "        return {\n",
    "            'up': (-1, 0),  # up,\n",
    "            'down': (1, 0),  # down,\n",
    "            'left': (0, -1),  # left,\n",
    "            'right': (0, 1),  # right,\n",
    "        }[action]\n",
    "\n",
    "    def get_transition_distribution(self, state, action):\n",
    "        # Discrete distributions, represented with a dict\n",
    "        # mapping next states to probs.\n",
    "        next_state_dist = defaultdict(float)\n",
    "\n",
    "        agent_pos, goal_pos = state\n",
    "\n",
    "        # Get next agent state\n",
    "        row, col = agent_pos\n",
    "        dr, dc = self.action_to_delta(action)\n",
    "        r, c = row + dr, col + dc\n",
    "        # Stay in place if out of bounds or obstacle\n",
    "        if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "            r, c = row, col\n",
    "        elif self.obstacles[r, c]:\n",
    "            r, c = row, col\n",
    "        next_agent_pos = (r, c)\n",
    "\n",
    "        # Get next bunny state\n",
    "        # Stay in same place with probability 0.5\n",
    "        next_state_dist[(next_agent_pos, goal_pos)] += 0.5\n",
    "        # Otherwise move\n",
    "        row, col = goal_pos\n",
    "        for (dr, dc) in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            r, c = row + dr, col + dc\n",
    "            # Stay in place if out of bounds or obstacle\n",
    "            if not (0 <= r < self.height and 0 <= c < self.width):\n",
    "                r, c = row, col\n",
    "            elif self.obstacles[r, c]:\n",
    "                r, c = row, col\n",
    "            next_goal_pos = (r, c)\n",
    "            next_state_dist[(next_agent_pos, next_goal_pos)] += 0.5*0.25\n",
    "\n",
    "        return next_state_dist\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        agent_pos, goal_pos = next_state\n",
    "        if agent_pos == goal_pos:\n",
    "            return self.goal_reward\n",
    "        return self.living_reward\n",
    "\n",
    "    def state_is_terminal(self, state):\n",
    "        agent_pos, goal_pos = state\n",
    "        return agent_pos == goal_pos\n",
    "\n",
    "\n",
    "class LargeChaseMDP(ChaseMDP):\n",
    "    \"\"\"A larger 2D grid bunny chasing MDP.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def obstacles(self):\n",
    "        return np.array([\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "            [0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        ])\n",
    "\n",
    "\n",
    "def rollout_Q(mdp, initial_state, Q, nr_simulations=10, max_steps=10):\n",
    "    \"\"\"\n",
    "    Simulate the policy for an MDP, given by a Q function.\n",
    "    At each step, we will greedily choose the action with the maximum Q value.\n",
    "    If the Q value hasn't been computed for this state, we will randomly choose an action.\n",
    "\n",
    "    Args:\n",
    "        mdp: An MDP instance.\n",
    "        initial_state: The initial state of the MDP.\n",
    "        Q: A dict, Q[state, action] -> Value\n",
    "        nr_simulations: An int. The number of simulations.\n",
    "        max_steps: An int. The maximum depth of the unrolling.\n",
    "\n",
    "    Return:\n",
    "        The average total reward over multiple simulations.\n",
    "    \"\"\"\n",
    "\n",
    "    total_reward = 0\n",
    "    for _ in range(nr_simulations):\n",
    "        s = initial_state\n",
    "        for j in range(max_steps):\n",
    "            if mdp.state_is_terminal(s):\n",
    "                break\n",
    "            not_initialized = any((s, a) not in Q for a in mdp.action_space)\n",
    "            if not_initialized:\n",
    "                a = np.random.choice(list(mdp.action_space))\n",
    "            else:\n",
    "                q_values = [(Q[s, a], a) for a in mdp.action_space]\n",
    "                a = max(q_values)[1]\n",
    "            ns = mdp.sample_next_state(s, a)\n",
    "            r = mdp.get_reward(s, a, ns)\n",
    "            total_reward += r\n",
    "            s = ns\n",
    "    return total_reward / nr_simulations\n",
    "\n",
    "\n",
    "def rollout_policy(mdp, initial_state, policy, nr_simulations=10, max_steps=10):\n",
    "    \"\"\"\n",
    "    Simulate the policy for an MDP, given by a policy.\n",
    "\n",
    "    Args:\n",
    "        mdp: An MDP instance.\n",
    "        initial_state: The initial state of the MDP.\n",
    "        policy: a Callable function, mapping from (mdp, current_state) to action.\n",
    "        Q: A dict, Q[state, action] -> Value\n",
    "        nr_simulations: An int. The number of simulations.\n",
    "        max_steps: An int. The maximum depth of the unrolling.\n",
    "\n",
    "    Return:\n",
    "        The average total reward over multiple simulations.\n",
    "    \"\"\"\n",
    "\n",
    "    total_reward = 0\n",
    "    for _ in range(nr_simulations):\n",
    "        s = initial_state\n",
    "        for j in range(max_steps):\n",
    "            if mdp.state_is_terminal(s):\n",
    "                break\n",
    "            a = policy(mdp, s)\n",
    "            ns = mdp.sample_next_state(s, a)\n",
    "            r = mdp.get_reward(s, a, ns)\n",
    "            total_reward += r\n",
    "            s = ns\n",
    "    return total_reward / nr_simulations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c264c",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17df235e",
   "metadata": {},
   "source": [
    "### Monte-Carlo Tree Search\n",
    "Complete the implementation of the Monte-Carlo Tree Search (MCTS) for an infinite or indefinite horizon MDP.\n",
    "\n",
    "For reference, our solution is **61** lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79263ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcts(mdp, initial_state, exploration_factor=1.0, iterations=100, max_depth=10):\n",
    "  \"\"\"Monte-Carlo Tree Search (MCTS) for solving an MDP.\n",
    "\n",
    "  Typically, an MCTS procedure keeps track of the running time of the algorithm\n",
    "  to determine when to return. Here, to simplify your implementation, your code\n",
    "  should run the simulation for `iterations` steps.\n",
    "\n",
    "  Your code should also take care of the maximum rollout steps to avoid infinite\n",
    "  looping. That is, the maximum depth of the search tree.\n",
    "  A typical way to handle this is to return 0 in the `simulate` function\n",
    "  when the maximum depth is reached.\n",
    "\n",
    "  You should implement the UCT (i.e., MCTS + UCB) for exploration.\n",
    "  The term `exploration_factor` is used to balance the Q value of a state-action\n",
    "  pair and the UCB term for that pair.\n",
    "\n",
    "  Args:\n",
    "      mdp: an MDP.\n",
    "      initial_state: the initial state (i.e., the root of the search tree).\n",
    "      exploration_factor: a floating-point number. It is the scalar hyperparameter\n",
    "          applied to the UCB term when choosing which action to expand.\n",
    "          In the lecture notes, this is refered to as c.\n",
    "      iterations: the number of iterations of `simulate`.\n",
    "      max_depth: the maximum depth during rolling-out.\n",
    "  \"\"\"\n",
    "  raise NotImplementedError(\"Implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25977546",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da889fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_mcts():\n",
    "    mdp = SingleRowMDP()\n",
    "    initial_state = 2\n",
    "    Q = mcts(mdp, initial_state)\n",
    "    for i in range(1, 4 + 1):\n",
    "        assert (i, 0) in Q and (i, 1) in Q\n",
    "        assert Q[i, 1] > Q[i, 0]\n",
    "\n",
    "test1_mcts()\n",
    "print('Tests passed.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
